# The AI Engineer â€” Capstone Projects

<p align="center">
  <img src="assets/tae_logo.png" width="160">
</p>

---

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white" alt="Python 3.11">
  <img src="https://img.shields.io/badge/Conda-Ready-green?logo=anaconda&logoColor=white" alt="Conda Ready">
  <img src="https://img.shields.io/badge/Colab-Friendly-brightgreen?logo=googlecolab&logoColor=white" alt="Colab Friendly">
  <img src="https://img.shields.io/badge/License-Educational%20Use-lightgrey" alt="Educational Use License">
  <img src="https://img.shields.io/badge/Last%20Updated-November%202025-purple" alt="Last Updated">
</p>

---

This repository contains my complete implementations of the capstone projects for *The AI Engineer* program (Nov 2025 Cohort).  
Each capstone is clean, reproducible, and aligned with softwareâ€‘engineering best practices.

---

## ğŸ“˜ Weekly Capstones Overview

| Week | Capstone | Summary | Colab Link |
|------|----------|---------|------------|
| **1** | **Gradient Descent Optimization** | Implement GD & SGD from scratch, analyze convergence, stepâ€‘size sensitivity, and basinâ€‘dependent dynamics. | [Open in Colab](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week01_gd_optimization/gd_capstone_final.ipynb) |
| **2** | **Backpropagation** | Manual chain rule, custom autograd, tiny MLP, PyTorch autograd, and nn.Module training loop. | [01](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week02_backprop/01_numpy_manual.ipynb) â€¢ [02](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week02_backprop/02_pytorch_no_autograd.ipynb) â€¢ [03](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week02_backprop/03_pytorch_autograd.ipynb) â€¢ [04](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week02_backprop/04_pytorch_nn_module.ipynb) |
| **3** | **Tiny Transformer** | Build tokenizer, attention, decoder block, training loop, inference sampling. | _Coming soon_ |
| **4** | **Agent Demo** | Minimal LLMâ€‘powered agent with clean abstractions, tracing, and monitoring. | _Coming soon_ |

---

## ğŸ“ Repository Structure

```
the_ai_engineer_capstones/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ tae_logo.png
â”‚
â””â”€â”€ capstones/
    â”œâ”€â”€ week01_gd_optimization/
    â”œâ”€â”€ week02_backprop/
    â”œâ”€â”€ week03_tiny_transformer/       (placeholder)
    â””â”€â”€ week04_agent_demo/             (placeholder)
```

---

## ğŸ”¬ Environment & Reproducibility

```
conda env create -f environment.yml
conda activate tae
```

- deterministic seeds  
- clean separation of logic/plots  
- fully programmatic datasets  
- each capstone selfâ€‘contained  

---

## âœ”ï¸ Status

**Week 1 â€” Completed**  
**Week 2 â€” Completed**  
Weeks 3â€“4 â€” *In progress*  
