# Gradient Descent Optimization Capstone â€” README

This folder contains the **Weekâ€‘1 Gradient Descent Optimization Capstone** for *The AI Engineer* program.

## ğŸ“Œ Overview

This notebook implements and explores gradientâ€‘based optimization on the nonâ€‘convex cubic function:

$$
f(x) = x^3 - 3x.
$$

It demonstrates:

- Deterministic Gradient Descent (GD)
- Stochastic Gradient Descent (SGD)
- Basinâ€‘dependent behavior
- Stepâ€‘size sensitivity
- Local linearization (tangentâ€‘line intuition)
- Reproducible experiments with fixed seeds

All figures are generated programmatically.

## ğŸ“ Contents

- `gd_capstone_final.ipynb` â€” main notebook
- All plots are generated at runtime (no hidden data).

## â–¶ï¸ Open in Google Colab

Click the badge below to launch the notebook directly in Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/week01_gd_optimization/gd_capstone_final.ipynb)

## ğŸ” Reproducibility

- A global `MASTER_SEED = 42` fixes RNG behavior.
- GD/SGD functions use deterministic update rules.
- All experiments run topâ€‘toâ€‘bottom without modification.

## ğŸš€ How to Run

1. Open in Colab using the badge above **or** clone the repo locally.
2. Install NumPy and Matplotlib (Colab already includes them).
3. Run the notebook sequentially â€” all plots and metrics are generated automatically.

## ğŸ“„ License

This capstone is provided for educational use under the terms of the repository license.
