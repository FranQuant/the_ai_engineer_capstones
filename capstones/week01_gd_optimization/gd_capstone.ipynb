{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90570b49-8522-4e37-b4d2-83b4ed754853",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week01_gd_optimization/gd_capstone.ipynb\n",
    ")\n",
    "\n",
    "# Gradient Descent Optimization — Week 1 Capstone\n",
    "\n",
    "This notebook implements the Week-1 capstone for *The AI Engineer* program using two one-dimensional objectives:\n",
    "\n",
    "1. **Quadratic baseline**\n",
    "   $$\n",
    "   q(x) = \\tfrac{1}{2} x^2\n",
    "   $$\n",
    "   A simple convex function used to study gradient descent (GD) stability and step-size effects.\n",
    "\n",
    "2. **Simple cubic loss**\n",
    "   $$\n",
    "   f(x) = x^3 - 3x\n",
    "   $$\n",
    "   A non-convex function with two stationary points, used to illustrate basins of attraction and GD/SGD dynamics.\n",
    "\n",
    "We focus on:\n",
    "\n",
    "- Deterministic **Gradient Descent (GD)**\n",
    "- **Stochastic Gradient Descent (SGD)** with:\n",
    "  - Constant step-size\n",
    "  - Diminishing schedule $\\eta_t = \\dfrac{\\eta_0}{1 + k t}$\n",
    "- **Step-size sensitivity** and stability\n",
    "- **Reproducible experiments** via a single global seed\n",
    "- Clear plots, short runtime, and Colab-ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b792e1-6c6f-4a96-bb06-cbaff2afbad7",
   "metadata": {},
   "source": [
    "## 0. Setup & Hyperparameters\n",
    "\n",
    "To ensure **deterministic, Colab-ready experiments**, we collect all global settings here:\n",
    "\n",
    "- A single **global NumPy seed** for reproducibility  \n",
    "- Centralized **hyperparameters** for GD and SGD  \n",
    "- Clean Matplotlib styling for consistent plots  \n",
    "- Notebook-wide tolerances and iteration limits  \n",
    "\n",
    "These values control **every experiment** below, ensuring that  \n",
    "each run produces identical results on Colab, local Jupyter, or GitHub Codespaces.\n",
    "\n",
    "### Global seed\n",
    "We fix a single integer:\n",
    "$$\n",
    "\\text{SEED} = 123\n",
    "$$\n",
    "\n",
    "### Learning rates\n",
    "For deterministic Gradient Descent:\n",
    "- Quadratic baseline GD uses a step-size sweep:\n",
    "  $$\n",
    "  \\eta \\in \\{0.05,\\; 0.10,\\; 0.15,\\; 0.20\\}\n",
    "  $$\n",
    "- Cubic loss GD uses:\n",
    "  $$\n",
    "  \\eta_{\\text{cubic}} = 0.05\n",
    "  $$\n",
    "\n",
    "### SGD settings\n",
    "We run both **constant-step** and **diminishing-step** schedules:\n",
    "\n",
    "- Constant step:\n",
    "  $$\n",
    "  \\eta = \\eta_0 = 0.05\n",
    "  $$\n",
    "\n",
    "- Diminishing step:\n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta_0}{1 + k t}, \\quad k = 0.01\n",
    "  $$\n",
    "\n",
    "### Experiment tolerances\n",
    "Objective-gap tolerance:\n",
    "$$\n",
    "\\varepsilon = 10^{-4}\n",
    "$$\n",
    "\n",
    "Maximum iterations:\n",
    "$$\n",
    "T_{\\max} = 5000\n",
    "$$\n",
    "\n",
    "### Notes\n",
    "- All figures include clear **titles, axis labels, and captions**.  \n",
    "- All experiments are **generated programmatically** (no external data).  \n",
    "- Runtime stays below **2 minutes**, as required for Week-1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005faf11-d659-4fbc-85ed-df104c8a5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Setup & Hyperparameters\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Reproducible Global Seed ---\n",
    "SEED = 123\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# --- Learning Rates ---\n",
    "# Quadratic baseline sweep (for stability analysis)\n",
    "ETA_QUAD_SWEEP = [0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "# GD on the cubic objective\n",
    "ETA_CUBIC = 0.05\n",
    "\n",
    "# --- SGD Settings ---\n",
    "ETA0 = 0.05            # constant-step SGD (η)\n",
    "ETA_SGD = ETA0         # alias for clarity inside SGD functions\n",
    "K_SCHEDULE = 0.01      # diminishing-step decay factor k\n",
    "SIGMA_SGD = 0.5        # Gaussian noise scale σ\n",
    "\n",
    "# --- Global Experiment Settings ---\n",
    "EPS = 1e-4             # convergence tolerance on objective gap\n",
    "T_MAX = 5000           # iteration cap for GD/SGD\n",
    "\n",
    "# --- Plot Styling ---\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "print(\"Setup complete — global seed, hyperparameters, and styling initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e9e73-3ad3-4df8-a5b3-d2cd1ca1dac3",
   "metadata": {},
   "source": [
    "## 1. Quadratic Baseline: Objective & Derivative\n",
    "\n",
    "We begin with the convex quadratic\n",
    "\n",
    "$$\n",
    "q(x) = \\tfrac{1}{2} x^2,\n",
    "$$\n",
    "\n",
    "which has a simple derivative\n",
    "\n",
    "$$\n",
    "q'(x) = x.\n",
    "$$\n",
    "\n",
    "This function is ideal for illustrating:\n",
    "\n",
    "- **Stability** vs **instability** under different step sizes  \n",
    "- Exact relationship between curvature and step-size limits  \n",
    "- How GD behaves in a perfectly smooth, convex landscape\n",
    "\n",
    "The unique minimizer is\n",
    "\n",
    "$$\n",
    "x^\\star = 0.\n",
    "$$\n",
    "\n",
    "Before running gradient descent, we implement the objective, derivative, and a helper function to compute the objective gap\n",
    "\n",
    "$$\n",
    "\\text{gap}(x_t) = q(x_t) - q(x^\\star).\n",
    "$$\n",
    "\n",
    "This will allow us to track convergence quantitatively in later sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c320520-8b34-4c36-b6a2-506233709d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Quadratic Baseline: Objective & Derivative\n",
    "# ============================================\n",
    "\n",
    "def q(x):\n",
    "    \"\"\"Quadratic objective: q(x) = 0.5 * x^2\"\"\"\n",
    "    return 0.5 * x**2\n",
    "\n",
    "def dq(x):\n",
    "    \"\"\"Derivative of the quadratic: q'(x) = x\"\"\"\n",
    "    return x\n",
    "\n",
    "def quad_gap(x):\n",
    "    \"\"\"Objective gap relative to optimum x* = 0.\"\"\"\n",
    "    return q(x) - q(0.0)\n",
    "\n",
    "print(\"Quadratic objective and derivative defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2f83-d278-43e1-ad44-89ffce40d33f",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent on the Quadratic — Step-Size Sweep\n",
    "\n",
    "For the quadratic\n",
    "$$\n",
    "q(x) = \\tfrac{1}{2}x^2,\n",
    "$$\n",
    "Gradient Descent follows the update rule\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta\\, q'(x_t) = x_t - \\eta x_t = (1 - \\eta)x_t.\n",
    "$$\n",
    "\n",
    "This yields an exact stability condition:\n",
    "$$\n",
    "|1 - \\eta| < 1 \\quad \\Longleftrightarrow \\quad 0 < \\eta < 2.\n",
    "$$\n",
    "\n",
    "A step-size sweep is ideal to illustrate:\n",
    "\n",
    "- **Slow convergence** for small $\\eta$\n",
    "- **Fast convergence** near the optimal $\\eta = 1$\n",
    "- **Oscillations** for $1 < \\eta < 2$\n",
    "- **Divergence** for $\\eta \\ge 2$\n",
    "\n",
    "We run GD from the same initialization\n",
    "$$\n",
    "x_0 = 4,\n",
    "$$\n",
    "and compare trajectories for:\n",
    "$$\n",
    "\\eta \\in \\{0.05,\\; 0.10,\\; 0.15,\\; 0.20\\}.\n",
    "$$\n",
    "\n",
    "A plot overlays these trajectories to show how step-size controls convergence speed and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7897db1-9d38-4428-a734-2a5fcfe4d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. GD on Quadratic — Step-Size Sweep\n",
    "# ============================================\n",
    "\n",
    "def gd_quadratic(x0, eta, T=T_MAX):\n",
    "    \"\"\"Runs GD on the quadratic for T steps.\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for _ in range(T):\n",
    "        x = x - eta * dq(x)   # dq(x) = x\n",
    "        xs.append(x)\n",
    "        if abs(x) < EPS:\n",
    "            break\n",
    "    return np.array(xs)\n",
    "\n",
    "# Initial point\n",
    "x0_quad = 4.0\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for eta in ETA_QUAD_SWEEP:\n",
    "    xs = gd_quadratic(x0_quad, eta)\n",
    "    plt.plot(xs, label=f\"η = {eta}\")\n",
    "\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.title(\"Quadratic GD — Step-Size Sweep\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.1,\n",
    "    \"GD trajectories for different step sizes on the quadratic objective.\\n\"\n",
    "    \"Small η converges slowly, η≈0.1 is faster, larger η approaches instability.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457781a-0c3d-4a00-9864-9b21cb598530",
   "metadata": {},
   "source": [
    "## 3. Cubic Objective: Non-Convex Landscape & Derivative\n",
    "\n",
    "We now introduce the non-convex cubic function\n",
    "\n",
    "$$\n",
    "f(x) = x^3 - 3x,\n",
    "$$\n",
    "\n",
    "which has derivative\n",
    "\n",
    "$$\n",
    "f'(x) = 3x^2 - 3.\n",
    "$$\n",
    "\n",
    "### Stationary points\n",
    "\n",
    "Set $f'(x) = 0$:\n",
    "\n",
    "$$\n",
    "3x^2 - 3 = 0 \\quad \\Longrightarrow \\quad x = \\pm 1.\n",
    "$$\n",
    "\n",
    "- **$x = -1$** is a **local maximum** (unstable under GD).  \n",
    "- **$x = +1$** is a **local minimum** (stable for $0 < \\eta < \\tfrac{1}{3}$).\n",
    "\n",
    "### Basins of attraction\n",
    "\n",
    "The cubic has only **one attracting basin** for positive learning rates:\n",
    "\n",
    "- If the initialization satisfies  \n",
    "  $$\n",
    "  x_0 > -1\n",
    "  $$  \n",
    "  then Gradient Descent converges to the stable minimizer at $x=1$.\n",
    "\n",
    "- If  \n",
    "  $$\n",
    "  x_0 < -1,\n",
    "  $$  \n",
    "  gradient descent diverges to $-\\infty$ unless a guard prevents it.\n",
    "\n",
    "- The point $x=-1$ is a **measure-zero unstable fixed point**.\n",
    "\n",
    "This makes the cubic ideal for illustrating:\n",
    "\n",
    "- **Stability vs. instability** of stationary points  \n",
    "- **Basin structure** in non-convex problems  \n",
    "- **Sensitivity to step size**  \n",
    "- Differences between GD and noisy **SGD**\n",
    "  \n",
    "We now implement the objective, derivative, and gap function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b6903-9e33-4073-a2a6-de2c06f2dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Cubic Objective: f(x) = x^3 - 3x\n",
    "# ============================================\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Cubic objective: f(x) = x^3 - 3x\"\"\"\n",
    "    return x**3 - 3*x\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 3x^2 - 3\"\"\"\n",
    "    return 3*x**2 - 3\n",
    "\n",
    "def cubic_gap(x):\n",
    "    \"\"\"Objective gap relative to the stable minimizer x* = +1.\"\"\"\n",
    "    return f(x) - f(1.0)\n",
    "\n",
    "print(\"Cubic objective and derivative defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89618-1b82-4243-a356-0b7ab7848d36",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent on the Cubic — Trajectories & Basins\n",
    "\n",
    "For the cubic\n",
    "\n",
    "$$\n",
    "f(x) = x^3 - 3x,\n",
    "\\quad\n",
    "f'(x) = 3x^2 - 3,\n",
    "$$\n",
    "\n",
    "Gradient Descent follows the update rule\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta f'(x_t).\n",
    "$$\n",
    "\n",
    "There are two stationary points:\n",
    "\n",
    "- **$x = -1$** — local maximum (repelling under GD)  \n",
    "- **$x = +1$** — local minimum (attracting when $0 < \\eta < 1/3$)\n",
    "\n",
    "### Basin structure\n",
    "\n",
    "The cubic has a **single attracting basin**:\n",
    "\n",
    "- If  \n",
    "  $$\n",
    "  x_0 > -1\n",
    "  $$  \n",
    "  GD converges to \\( x^\\star = 1 \\).\n",
    "\n",
    "- If  \n",
    "  $$\n",
    "  x_0 < -1,\n",
    "  $$  \n",
    "  GD diverges to \\( -\\infty \\) unless a guard prevents it.\n",
    "\n",
    "We run GD from several initial points to visualize this behavior:\n",
    "\n",
    "$$\n",
    "x_0 \\in \\{-3,\\; -0.5,\\; 0.5,\\; 2\\}.\n",
    "$$\n",
    "\n",
    "We use the fixed step-size:\n",
    "\n",
    "$$\n",
    "\\eta_{\\text{cubic}} = 0.05.\n",
    "$$\n",
    "\n",
    "The resulting trajectories reveal the basin boundary at \\( x = -1 \\), the stable attractor at \\( x = 1 \\), and divergence when starting too far left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2453fd-60d7-4818-b618-ef89a3535e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. GD on the Cubic — Multiple Initializations\n",
    "# ============================================\n",
    "\n",
    "def gd_cubic(x0, eta=ETA_CUBIC, T=T_MAX):\n",
    "    \"\"\"Runs GD on the cubic for T steps (with guard for divergence).\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for _ in range(T):\n",
    "        x = x - eta * df(x)\n",
    "\n",
    "        # Divergence guard to avoid blowing up the plot\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "# Initializations to test\n",
    "inits = [-3.0, -0.5, 0.5, 2.0]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for x0 in inits:\n",
    "    xs = gd_cubic(x0, ETA_CUBIC)\n",
    "    plt.plot(xs[:50], label=f\"x0 = {x0}\")\n",
    "\n",
    "plt.axhline(1.0, color=\"black\", linewidth=1, linestyle=\"--\", label=\"x* = 1\")\n",
    "plt.title(\"GD on the Cubic — Basin Behavior\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.1,\n",
    "    \"GD trajectories on the cubic. Initializations right of -1 converge to x*=1; \"\n",
    "    \"starting left of -1 diverges without guards.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4eeeec-c780-4441-a71b-6f3e60f4bd09",
   "metadata": {},
   "source": [
    "## 5. Stochastic Gradient Descent (SGD) on the Cubic\n",
    "\n",
    "Stochastic Gradient Descent introduces noise into the gradient evaluation.  \n",
    "Instead of using the exact derivative  \n",
    "$$\n",
    "f'(x) = 3x^2 - 3,\n",
    "$$  \n",
    "SGD uses a noisy estimate:\n",
    "$$\n",
    "g_t = f'(x_t) + \\varepsilon_t, \n",
    "\\qquad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "This produces characteristic **wandering** around the stable minimizer \\(x^\\star = 1\\), with a  \n",
    "steady-state radius proportional to \\(\\sqrt{\\eta \\sigma^2}\\).\n",
    "\n",
    "We implement two SGD schedules:\n",
    "\n",
    "1. **Constant step-size**\n",
    "   $$\n",
    "   x_{t+1} = x_t - \\eta \\, g_t\n",
    "   $$\n",
    "   This converges only to a *neighborhood* of \\(x^\\star\\).\n",
    "\n",
    "2. **Diminishing step-size**\n",
    "   $$\n",
    "   \\eta_t = \\frac{\\eta_0}{1 + k t}\n",
    "   $$\n",
    "   $$\n",
    "   x_{t+1} = x_t - \\eta_t \\, g_t\n",
    "   $$\n",
    "   This gradually shrinks noise and yields a tighter neighborhood around the minimizer.\n",
    "\n",
    "We now define both SGD variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136beb3-d680-4ee3-8c6c-2595e7a4eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. SGD on the Cubic — Constant & Diminishing Step\n",
    "# ============================================\n",
    "\n",
    "def sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_MAX):\n",
    "    \"\"\"\n",
    "    SGD with constant step size:\n",
    "        x_{t+1} = x_t - eta * (df(x_t) + noise)\n",
    "    \"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    for _ in range(T):\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta * (df(x) + noise)\n",
    "\n",
    "        # Divergence guard (keeps plots readable)\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "\n",
    "def sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_MAX):\n",
    "    \"\"\"\n",
    "    SGD with diminishing step size:\n",
    "        eta_t = eta0 / (1 + k t)\n",
    "        x_{t+1} = x_t - eta_t * (df(x_t) + noise)\n",
    "    \"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    for t in range(T):\n",
    "        eta_t = eta0 / (1.0 + k * t)\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta_t * (df(x) + noise)\n",
    "\n",
    "        # Divergence guard\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "print(\"SGD functions (constant and diminishing schedules) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a393af0-84f2-49ae-aa62-d9646ee24727",
   "metadata": {},
   "source": [
    "## 6. SGD Trajectories on the Cubic\n",
    "\n",
    "We now compare **constant-step** and **diminishing-step** SGD on the cubic objective  \n",
    "starting from the same initialization \\( x_0 = 0.5 \\).\n",
    "\n",
    "- With a **constant step**, SGD converges only to a *noisy neighborhood* around  \n",
    "  $ x^\\star = 1 $, with fluctuations scaling like $ \\sqrt{\\eta \\sigma^2} $.\n",
    "\n",
    "- With a **diminishing schedule**  \n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta_0}{1 + k t},\n",
    "  $$\n",
    "  the noise gradually shrinks, resulting in a tighter approach to the minimizer.\n",
    "\n",
    "The plot below compares both trajectories over $ T = 200 $ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8fb9a-3b2b-4fa5-8dd6-a0ce822fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. SGD Trajectories — Constant vs Diminishing\n",
    "# ============================================\n",
    "\n",
    "T_plot = 200\n",
    "x0 = 0.5\n",
    "\n",
    "xs_const = sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_plot)\n",
    "xs_dimin = sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_plot)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(xs_const, label=\"Constant SGD\", linewidth=2)\n",
    "plt.plot(xs_dimin, label=\"Diminishing-step SGD\", linewidth=2)\n",
    "plt.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"x* = 1\")\n",
    "\n",
    "plt.title(\"SGD on the Cubic — Constant vs Diminishing Step\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.12,\n",
    "    \"Constant-step SGD fluctuates around x*=1; diminishing-step SGD converges tighter as eta_t decreases.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380ea1d-98a2-4b26-8a53-9fa24046aed0",
   "metadata": {},
   "source": [
    "## 7. Metrics: Final Gap, Best Gap, and Steps-to-Tolerance\n",
    "\n",
    "For each optimization method, we report three quantities relative to the cubic\n",
    "objective’s stable minimizer at \\(x^\\star = 1\\):\n",
    "\n",
    "- **Final gap:**  \n",
    "  $$\n",
    "  | f(x_T) - f(x^\\star) |\n",
    "  $$\n",
    "\n",
    "- **Best gap:**  \n",
    "  $$\n",
    "  \\min_{t \\le T} \\; |f(x_t) - f(x^\\star)| \n",
    "  $$\n",
    "\n",
    "- **Steps-to-tolerance:**  \n",
    "  The first iteration \\(t\\) such that  \n",
    "  $$\n",
    "  |f(x_t) - f(x^\\star)| < \\varepsilon, \\qquad \\varepsilon = 10^{-4}.\n",
    "  $$\n",
    "\n",
    "These metrics quantify convergence speed and stability for GD, constant-step SGD,\n",
    "and diminishing-step SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb69baf-f9a5-4cfa-a196-82c32baaf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Metrics: Gaps and Steps-to-Tolerance\n",
    "# ============================================\n",
    "\n",
    "def compute_metrics(xs, objective, x_star=1.0, eps=EPS):\n",
    "    \"\"\"Compute final gap, best gap, and steps-to-tolerance.\"\"\"\n",
    "    gaps = np.abs(objective(xs) - objective(x_star))\n",
    "\n",
    "    final_gap = gaps[-1]\n",
    "    best_gap = gaps.min()\n",
    "\n",
    "    # First iteration where gap < eps (if any)\n",
    "    idx = np.where(gaps < eps)[0]\n",
    "    steps_to_tol = int(idx[0]) if len(idx) > 0 else None\n",
    "\n",
    "    return final_gap, best_gap, steps_to_tol\n",
    "\n",
    "\n",
    "# ----- Run trajectories -----\n",
    "\n",
    "# GD\n",
    "xs_gd = gd_cubic(0.5, ETA_CUBIC)\n",
    "\n",
    "# SGD constant\n",
    "xs_sgd_const = sgd_constant(0.5, eta=ETA_SGD, sigma=SIGMA_SGD)\n",
    "\n",
    "# SGD diminishing\n",
    "xs_sgd_dim = sgd_diminishing(0.5, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD)\n",
    "\n",
    "\n",
    "# ----- Compute metrics -----\n",
    "\n",
    "gd_metrics     = compute_metrics(xs_gd, f)\n",
    "sgd_c_metrics  = compute_metrics(xs_sgd_const, f)\n",
    "sgd_d_metrics  = compute_metrics(xs_sgd_dim, f)\n",
    "\n",
    "\n",
    "# ----- Display -----\n",
    "\n",
    "print(\"=== Convergence Metrics (Cubic Objective) ===\")\n",
    "print(f\"GD (eta={ETA_CUBIC}):\")\n",
    "print(f\"  Final gap        = {gd_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {gd_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {gd_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Constant Step:\")\n",
    "print(f\"  Final gap        = {sgd_c_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_c_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_c_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Diminishing Step:\")\n",
    "print(f\"  Final gap        = {sgd_d_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_d_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_d_metrics[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4a564-c8c0-43b9-809f-ddedd8f3c8d6",
   "metadata": {},
   "source": [
    "## 8. Final Commentary — What We Learned\n",
    "\n",
    "**Gradient Descent (GD):**  \n",
    "With the cubic objective, GD converges rapidly and cleanly to the stable minimizer \n",
    "$ x^\\star = 1 $.  \n",
    "For η = 0.05, convergence occurs in ~14 iterations, with monotone improvement and no noise.\n",
    "This matches the textbook behavior on smooth, strongly-curved regions of the landscape.\n",
    "\n",
    "**SGD with Constant Step Size:**  \n",
    "SGD replaces the exact gradient with a noisy estimate.  \n",
    "With a fixed learning rate, the algorithm does not converge to a single point but instead\n",
    "wanders in a stationary “noise bubble” around the minimizer.  \n",
    "The radius of this steady-state cloud scales with $ \\sqrt{\\eta \\sigma^2} $.  \n",
    "Occasionally the noise pushes the iterate closer to the minimizer than GD ever reaches,\n",
    "which explains why the *best gap* can be extremely small even when the final gap is not.\n",
    "\n",
    "**SGD with Diminishing Step Size:**  \n",
    "With a schedule  \n",
    "$ \\eta_t = \\eta_0 / (1 + k t) $,  \n",
    "the noise becomes progressively weaker.  \n",
    "Early iterations resemble constant-step SGD; later ones resemble GD.  \n",
    "This produces slow but true convergence toward $x^\\star$.\n",
    "\n",
    "**Overall Lessons:**  \n",
    "- GD is fast and stable but rigid.  \n",
    "- Constant-step SGD explores the landscape but cannot converge to a point.  \n",
    "- Diminishing-step SGD converges but more slowly.  \n",
    "- Noise can both help (exploration) and harm (steady-state variability).  \n",
    "\n",
    "These behaviors mirror the fundamental tradeoffs in modern large-scale optimization,\n",
    "where SGD and its variants power deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
