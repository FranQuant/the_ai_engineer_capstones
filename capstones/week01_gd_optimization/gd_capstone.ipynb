{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90570b49-8522-4e37-b4d2-83b4ed754853",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week01_gd_optimization/gd_capstone.ipynb\n",
    ")\n",
    "\n",
    "# Gradient Descent Optimization — Week 1 Capstone\n",
    "\n",
    ">This notebook implements the Week-1 capstone for *The AI Engineer* program using two 1-D objectives.\n",
    "\n",
    "1. **Quadratic baseline**\n",
    "   $$\n",
    "   q(x) = \\tfrac{1}{2} x^2\n",
    "   $$\n",
    "   A convex reference for studying GD stability and step-size behavior.\n",
    "\n",
    "2. **Cubic loss**\n",
    "   $$\n",
    "   f(x) = x^3 - 3x\n",
    "   $$\n",
    "   A non-convex function highlighting multiple stationary points and contrasting GD/SGD dynamics.\n",
    "\n",
    "We analyze:\n",
    "- Deterministic **GD**\n",
    "- **SGD** (constant and diminishing steps: $\\eta_t = \\frac{\\eta_0}{1 + k t}$)\n",
    "- **Step-size sensitivity**, stability regimes, and reproducibility via a fixed global seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2459a-8269-406e-86d9-72ff9fd7ff7a",
   "metadata": {},
   "source": [
    "## 1. Setup & Hyperparameters\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Component | Setting | Value / Formula |\n",
    "|----------|---------|------------------|\n",
    "| **Global seed** | Fixed integer | $\\text{SEED} = 123$ |\n",
    "| **GD step-sizes (quadratic)** | Sweep | $\\eta \\in \\{0.05,\\;0.10,\\;0.15,\\;0.20\\}$ (plus $\\eta=2.1$ for an unstable example) |\n",
    "| **GD step-size (cubic)** | Fixed | $\\eta_{\\text{cubic}} = 0.05$ |\n",
    "| **SGD (constant)** | Step-size | $\\eta = \\eta_0 = 0.05$ |\n",
    "| **SGD (diminishing)** | Schedule | $\\eta_t = \\dfrac{\\eta_0}{1 + k t}$ |\n",
    "| **SGD decay constant** | $k$ | $k = 0.01$ |\n",
    "| **Tolerance** | Objective-gap | $\\varepsilon = 10^{-4}$ |\n",
    "| **Max iterations** | Upper bound | $T_{\\max} = 5000$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005faf11-d659-4fbc-85ed-df104c8a5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Setup & Hyperparameters\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Reproducible Global Seed ---\n",
    "SEED = 123\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# --- Learning Rates ---\n",
    "# Quadratic baseline sweep (for stability analysis)\n",
    "ETA_QUAD_SWEEP = [0.05, 0.10, 0.15, 0.20, 2.1] # 2.1 is outside 0<η<2, so GD diverges\n",
    "\n",
    "# GD on the cubic objective\n",
    "ETA_CUBIC = 0.05\n",
    "\n",
    "# --- SGD Settings ---\n",
    "ETA0 = 0.05            # constant-step SGD (η)\n",
    "ETA_SGD = ETA0         # alias for clarity inside SGD functions\n",
    "K_SCHEDULE = 0.01      # diminishing-step decay factor k\n",
    "SIGMA_SGD = 0.5        # Gaussian noise scale σ\n",
    "\n",
    "# --- Global Experiment Settings ---\n",
    "EPS = 1e-4             # convergence tolerance on objective gap\n",
    "T_MAX = 5000           # iteration cap for GD/SGD\n",
    "\n",
    "# --- Plot Styling ---\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e9e73-3ad3-4df8-a5b3-d2cd1ca1dac3",
   "metadata": {},
   "source": [
    "## 2. Quadratic Baseline: Objective & Derivative\n",
    "\n",
    "We begin with the convex quadratic\n",
    "\n",
    "$$\n",
    "q(x) = \\tfrac{1}{2} x^2,\n",
    "$$\n",
    "\n",
    "which has a simple derivative\n",
    "\n",
    "$$\n",
    "q'(x) = x.\n",
    "$$\n",
    "\n",
    "This function is ideal for illustrating:\n",
    "\n",
    ">- **Stability** vs **instability** under different step sizes  \n",
    ">- Exact relationship between curvature and step-size limits  \n",
    ">- How GD behaves in a perfectly smooth, convex landscape\n",
    "\n",
    "The unique minimizer is\n",
    "\n",
    "$$\n",
    "x^\\star = 0.\n",
    "$$\n",
    "\n",
    "> Before running gradient descent, we implement the objective, derivative, and a helper function to compute the objective gap\n",
    "\n",
    "$$\n",
    "\\text{gap}(x_t) = q(x_t) - q(x^\\star).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c320520-8b34-4c36-b6a2-506233709d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Quadratic Baseline: Objective & Derivative\n",
    "# ============================================\n",
    "\n",
    "def q(x):\n",
    "    \"\"\"Quadratic objective: q(x) = 0.5 * x^2\"\"\"\n",
    "    return 0.5 * x**2\n",
    "\n",
    "def dq(x):\n",
    "    \"\"\"Derivative of the quadratic: q'(x) = x\"\"\"\n",
    "    return x\n",
    "\n",
    "def quad_gap(x):\n",
    "    \"\"\"Objective gap relative to optimum x* = 0.\"\"\"\n",
    "    return q(x) - q(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2f83-d278-43e1-ad44-89ffce40d33f",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent on the Quadratic — Step-Size Sweep\n",
    "\n",
    ">For the quadratic\n",
    ">$$q(x) = \\tfrac{1}{2}x^2$$\n",
    ">Gradient Descent follows the update rule\n",
    ">$$x_{t+1} = x_t - \\eta\\, q'(x_t) = x_t - \\eta x_t = (1 - \\eta)x_t$$\n",
    ">\n",
    ">This yields an exact stability condition:\n",
    ">$$|1 - \\eta| < 1 \\quad \\Longleftrightarrow \\quad 0 < \\eta < 2$$\n",
    "\n",
    "A step-size sweep is ideal to illustrate:\n",
    "\n",
    ">- **Slow convergence** for small $\\eta$\n",
    ">- **Fast convergence** near the optimal $\\eta = 1$\n",
    ">- **Oscillations** for $1 < \\eta < 2$\n",
    ">- **Divergence** for $\\eta \\ge 2$\n",
    ">\n",
    ">We run GD from the same initialization\n",
    ">$$x_0 = 4$$\n",
    ">and compare trajectories for:\n",
    ">$$\\eta \\in \\{0.05,\\; 0.10,\\; 0.15,\\; 0.20, 2.1\\}$$\n",
    ">\n",
    ">We include an additional unstable step size:\n",
    ">\n",
    ">- Stable: 0.05, 0.10, 0.15, 0.20  \n",
    ">- Unstable: **2.1** → divergence (violates $0 < \\eta < 2$)\n",
    ">  \n",
    ">GD on a quadratic with curvature 1 is stable only if:\n",
    ">$$0 < \\eta < 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7897db1-9d38-4428-a734-2a5fcfe4d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. GD on Quadratic — Step-Size Sweep (Two-Pane Figure)\n",
    "# ============================================\n",
    "\n",
    "def gd_quadratic(x0, eta, T=T_MAX):\n",
    "    \"\"\"Runs GD on the quadratic for T steps (with gap-based stopping + divergence guard).\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "\n",
    "    for _ in range(T):\n",
    "        x = x - eta * dq(x)\n",
    "\n",
    "        # --- Divergence guard (critical!) ---\n",
    "        if abs(x) > 1e6:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "        # --- Gap-based stopping ---\n",
    "        if abs(q(x) - q(0.0)) < EPS:\n",
    "            break\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "# ============================================\n",
    "# 2. GD on the Quadratic — Stable vs Unstable\n",
    "# ============================================\n",
    "\n",
    "x0_quad = 4.0\n",
    "stable_etas = [0.05, 0.10, 0.15, 0.20]\n",
    "unstable_eta = 2.1\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# --- Top panel: stable ---\n",
    "for eta in stable_etas:\n",
    "    xs = gd_quadratic(x0_quad, eta)\n",
    "    ax[0].plot(xs, label=f\"η = {eta}\")\n",
    "\n",
    "ax[0].set_title(\"Quadratic GD — Stable Step Sizes (0 < η < 2)\")\n",
    "ax[0].set_xlabel(\"Iteration\")\n",
    "ax[0].set_ylabel(\"x_t\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- Bottom panel: divergent ---\n",
    "xs_div = gd_quadratic(x0_quad, unstable_eta)\n",
    "ax[1].plot(xs_div, color=\"purple\", label=f\"η = {unstable_eta}\")\n",
    "\n",
    "ax[1].set_title(\"Quadratic GD — Divergence (η ≥ 2)\")\n",
    "ax[1].set_xlabel(\"Iteration\")\n",
    "ax[1].set_ylabel(\"x_t\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.02,\n",
    "    \"Stable η values decay smoothly; η = 2.1 lies outside 0 < η < 2 and diverges geometrically.\",\n",
    "    ha=\"center\", fontsize=10,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f18f8-d068-46e5-88b1-b2690d9aef35",
   "metadata": {},
   "source": [
    "Thus, $\\eta = 0.05, 0.10, 0.15, 0.20$ all converge to $x^\\star = 0$ (with faster convergence\n",
    "as $\\eta$ grows), while $\\eta = 2.1$ lies outside the stability band and produces true divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb9da5-fc87-410d-ad76-19816020a4b0",
   "metadata": {},
   "source": [
    "### Quadratic GD — Convergence Metrics\n",
    "\n",
    "We compute simple convergence diagnostics for the quadratic baseline.\n",
    "\n",
    "- **Final gap**  \n",
    "  $$|q(x_T) - q(0)|$$\n",
    "\n",
    "- **Best gap**  \n",
    "  $$\\min_{t \\le T} |q(x_t) - q(0)|$$\n",
    "\n",
    "- **Steps-to-tolerance**  \n",
    "  $$|q(x_t) - q(0)| < 10^{-4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4853c9-d310-4505-ac19-85b33bbe199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Quadratic Metrics\n",
    "# ============================================\n",
    "\n",
    "def compute_quad_metrics(xs, eps=EPS):\n",
    "    \"\"\"Compute final gap, best gap, and steps-to-tolerance for q(x).\"\"\"\n",
    "    gaps = np.abs(q(xs) - q(0.0))\n",
    "\n",
    "    final_gap = gaps[-1]\n",
    "    best_gap = gaps.min()\n",
    "\n",
    "    idx = np.where(gaps < eps)[0]\n",
    "    steps_to_tol = int(idx[0]) if len(idx) > 0 else None\n",
    "\n",
    "    return final_gap, best_gap, steps_to_tol\n",
    "\n",
    "\n",
    "# Run GD on the quadratic with a representative step size\n",
    "xs_quad_metrics = gd_quadratic(4.0, eta=0.05)\n",
    "\n",
    "quad_metrics = compute_quad_metrics(xs_quad_metrics)\n",
    "\n",
    "print(\"=== Convergence Metrics (Quadratic Objective) ===\")\n",
    "print(f\"GD (eta=0.05):\")\n",
    "print(f\"  Final gap        = {quad_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {quad_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {quad_metrics[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457781a-0c3d-4a00-9864-9b21cb598530",
   "metadata": {},
   "source": [
    "## 4. Cubic Objective: Non-Convex Landscape & Derivative\n",
    "\n",
    "We now introduce the non-convex cubic function\n",
    "\n",
    "> $$f(x) = x^3 - 3x,\\quad f'(x) = 3x^2 - 3.$$\n",
    "\n",
    "\n",
    "### Stationary points\n",
    "\n",
    ">Set $f'(x) = 0$:\n",
    ">\n",
    ">$$3x^2 - 3 = 0 \\quad \\Longrightarrow \\quad x = \\pm 1$$\n",
    ">\n",
    ">- **$x = -1$** is a **local maximum** (unstable under GD).  \n",
    ">- **$x = +1$** is a **local minimum** (stable for $0 < \\eta < \\tfrac{1}{3}$).\n",
    "\n",
    "### Basin structure\n",
    "\n",
    ">For positive step sizes:\n",
    ">\n",
    ">- Ix  $x_0 > -1$  GD flows to the stable minimizer $x=1$\n",
    ">- If $x_0 < -1$, the gradient pushes the iterate toward $-\\infty$ unless a guard prevents it.\n",
    ">\n",
    ">- The point $x=-1$ is a **unstable equilibrium of measure zero**.\n",
    "\n",
    "**This makes the cubic ideal for illustrating:**\n",
    "\n",
    ">- **Stability vs. instability** of critical points  \n",
    ">- **Basymmetric basins of attraction**  \n",
    ">- **Sensitivity to initializatione**  \n",
    ">- Differences between GD and noisy **SGD**\n",
    "  \n",
    "Below we implement the function, gradient, and the objective-gap around the minimizer at $x^\\star = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b6903-9e33-4073-a2a6-de2c06f2dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Cubic Objective: f(x) = x^3 - 3x\n",
    "# ============================================\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Cubic objective: f(x) = x^3 - 3x\"\"\"\n",
    "    return x**3 - 3*x\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 3x^2 - 3\"\"\"\n",
    "    return 3*x**2 - 3\n",
    "\n",
    "def cubic_gap(x):\n",
    "    \"\"\"Objective gap relative to the stable minimizer x* = +1.\"\"\"\n",
    "    return f(x) - f(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5dfd9-e113-4242-9d5c-c8885d612f4e",
   "metadata": {},
   "source": [
    "### Cubic Objective — Function and Derivative\n",
    "*We briefly inspect the cubic landscape to capture its non-convex geometry before running optimization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc2e05-f52b-4bec-8f5a-8c673c5af1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cubic Diagnostics: f(x) and f'(x)\n",
    "# ============================================\n",
    "\n",
    "xs = np.linspace(-3, 3, 400)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# --- f(x) plot ---\n",
    "ax[0].plot(xs, f(xs), linewidth=2)\n",
    "ax[0].axvline(-1, linestyle=\"--\", color=\"grey\", label=\"x = -1 (max)\")\n",
    "ax[0].axvline(1, linestyle=\"--\", color=\"black\", label=\"x = 1 (min)\")\n",
    "ax[0].set_title(\"Cubic Objective  $f(x) = x^3 - 3x$\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"f(x)\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- f'(x) plot ---\n",
    "ax[1].plot(xs, df(xs), linewidth=2, color=\"darkorange\")\n",
    "ax[1].axhline(0, color=\"black\", linewidth=1)\n",
    "ax[1].axvline(-1, linestyle=\"--\", color=\"grey\")\n",
    "ax[1].axvline(1, linestyle=\"--\", color=\"black\")\n",
    "ax[1].set_title(\"Derivative  $f'(x) = 3x^2 - 3$\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"f'(x)\")\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,\n",
    "    \"The cubic has a saddle-like region around x=-1 and strong curvature near x=±∞.\\n\"\n",
    "    \"This explains sensitivity of GD to initialization and the steady-state variance of SGD.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89618-1b82-4243-a356-0b7ab7848d36",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent on the Cubic — Trajectories & Basins\n",
    "\n",
    "For the cubic\n",
    "\n",
    "$$\n",
    "f(x) = x^3 - 3x,\n",
    "\\quad\n",
    "f'(x) = 3x^2 - 3,\n",
    "$$\n",
    ">\n",
    ">Gradient Descent follows the update rule\n",
    ">\n",
    ">$$x_{t+1} = x_t - \\eta f'(x_t)$$\n",
    ">\n",
    ">There are two stationary points:\n",
    ">\n",
    ">- **$x = -1$** — local maximum (repelling under GD)  \n",
    ">- **$x = +1$** — local minimum (attracting when $0 < \\eta < 1/3$)\n",
    "\n",
    "### Basin structure\n",
    "\n",
    "The cubic has a **single attracting basin**:\n",
    "\n",
    ">- If $x_0 > -1$\n",
    ">\n",
    ">- GD converges to $ x^\\star = 1 $.\n",
    ">\n",
    ">- If $x_0 < -1,$\n",
    ">\n",
    ">  GD diverges to $ -\\infty $ unless a guard prevents it.\n",
    "\n",
    "We run GD from several initial points to visualize this behavior:\n",
    "\n",
    ">$$x_0 \\in \\{-3,\\; -0.5,\\; 0.5,\\; 2,\\; 1.5\\}$$\n",
    ">\n",
    ">We use the fixed step-size:\n",
    ">\n",
    ">$$\\eta_{\\text{cubic}} = 0.05.$$\n",
    "\n",
    "The resulting trajectories reveal the basin boundary at $x = -1$, the stable attractor at $x = 1$, and divergence when starting too far left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2453fd-60d7-4818-b618-ef89a3535e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. GD on the Cubic — Multiple Initializations\n",
    "# ============================================\n",
    "\n",
    "def gd_cubic(x0, eta=ETA_CUBIC, T=T_MAX):\n",
    "    \"\"\"Runs GD on the cubic for T steps (with guard for divergence and gap-based early stop).\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "\n",
    "    for _ in range(T):\n",
    "        x = x - eta * df(x)\n",
    "\n",
    "        # ---- GAP-BASED STOPPING (relative to minimum at x*=1) ----\n",
    "        if abs(f(x) - f(1.0)) < EPS:      # This is the spec requirement\n",
    "            xs.append(x)\n",
    "            break\n",
    "\n",
    "        # ---- DIVERGENCE GUARD ----\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "# Initializations to test\n",
    "inits = [-3.0, -0.5, 0.5, 2.0, 1.5]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for x0 in inits:\n",
    "    xs = gd_cubic(x0, ETA_CUBIC)\n",
    "    plt.plot(xs[:20], label=f\"x0 = {x0}\")\n",
    "\n",
    "plt.axhline(1.0, color=\"black\", linewidth=1, linestyle=\"--\", label=\"x* = 1\")\n",
    "plt.title(\"GD on the Cubic — Basin Behavior\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.1,\n",
    "    \"GD trajectories on the cubic. Initializations right of -1 converge to x*=1; \"\n",
    "    \"starting left of -1 diverges without guards.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adab1d3-ffe0-49ac-8c76-5ca1f49cf1ff",
   "metadata": {},
   "source": [
    "## 5. Stochastic Gradient Descent (SGD) on the Cubic\n",
    "\n",
    "SGD replaces the exact gradient  \n",
    "$$f'(x) = 3x^2 - 3$$  \n",
    "with a noisy estimate:\n",
    "$$\n",
    "g_t = f'(x_t) + \\varepsilon_t, \n",
    "\\qquad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "\n",
    "This induces **wandering** around the minimizer $x^\\star = 1$, with steady-state variance\n",
    "$$\n",
    "\\mathrm{Var}(x_t) \\propto \\eta\\,\\sigma^2,\n",
    "\\qquad \\text{radius} \\sim \\sqrt{\\eta}\\,\\sigma.\n",
    "$$\n",
    "\n",
    "We use two SGD schedules:\n",
    "\n",
    "1. **Constant step-size**  \n",
    "   $$x_{t+1} = x_t - \\eta\\, g_t$$  \n",
    "   Converges only to a *neighborhood* of $x^\\star$.\n",
    "\n",
    "2. **Diminishing step-size**  \n",
    "   $$\\eta_t = \\frac{\\eta_0}{1 + k t}$$  \n",
    "   $$x_{t+1} = x_t - \\eta_t\\, g_t$$  \n",
    "   Gradually reduces noise and tightens convergence around the minimizer.\n",
    "\n",
    "Both variants rely on a shared global RNG for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136beb3-d680-4ee3-8c6c-2595e7a4eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. SGD on the Cubic — Constant & Diminishing Step (Shared RNG Version)\n",
    "# ============================================\n",
    "\n",
    "def sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_MAX, rng=rng):\n",
    "    \"\"\"SGD with constant step size + gap-based stopping.\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "\n",
    "    for _ in range(T):\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta * (df(x) + noise)\n",
    "\n",
    "        # ---- GAP-BASED STOPPING ----\n",
    "        if abs(f(x) - f(1.0)) < EPS:\n",
    "            xs.append(x)\n",
    "            break\n",
    "\n",
    "        # ---- DIVERGENCE GUARD ----\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "\n",
    "def sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_MAX, rng=rng):\n",
    "    \"\"\"SGD with diminishing steps + gap-based stopping.\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "\n",
    "    for t in range(T):\n",
    "        eta_t = eta0 / (1.0 + k * t)\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta_t * (df(x) + noise)\n",
    "\n",
    "        # ---- GAP-BASED STOPPING ----\n",
    "        if abs(f(x) - f(1.0)) < EPS:\n",
    "            xs.append(x)\n",
    "            break\n",
    "\n",
    "        # ---- DIVERGENCE GUARD ----\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd2695-65cb-406d-9dfa-09924f87a919",
   "metadata": {},
   "source": [
    "## 6. SGD Trajectories on the Cubic\n",
    "\n",
    "We now compare **constant-step** and **diminishing-step** SGD on the cubic objective  \n",
    "starting from the same initialization $x_0 = 0.5$.\n",
    "\n",
    "- With a **constant step**, SGD converges only to a *noisy neighborhood* around  \n",
    "  $x^\\star = 1$, with fluctuations scaling like  \n",
    "  $$\\text{radius} \\propto \\eta^{1/2}\\,\\sigma.$$\n",
    "\n",
    "- With a **diminishing schedule**  \n",
    "  $$\\eta_t = \\frac{\\eta_0}{1 + k t},$$  \n",
    "  the noise gradually shrinks, yielding a tighter approach to the minimizer.\n",
    "\n",
    "The plot below compares both trajectories over $T = 200$ iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8fb9a-3b2b-4fa5-8dd6-a0ce822fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. SGD Trajectories — Constant vs Diminishing\n",
    "# ============================================\n",
    "\n",
    "T_plot = 200\n",
    "x0 = 0.5\n",
    "\n",
    "xs_const = sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_plot)\n",
    "xs_dimin = sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_plot)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(xs_const, label=\"Constant SGD\", linewidth=2)\n",
    "plt.plot(xs_dimin, label=\"Diminishing-step SGD\", linewidth=2)\n",
    "plt.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"x* = 1\")\n",
    "\n",
    "plt.title(\"SGD on the Cubic — Constant vs Diminishing Step\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.12,\n",
    "    \"Constant-step SGD fluctuates around x*=1; diminishing-step SGD converges tighter as eta_t decreases.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380ea1d-98a2-4b26-8a53-9fa24046aed0",
   "metadata": {},
   "source": [
    "## 7. Metrics: Final Gap, Best Gap, and Steps-to-Tolerance\n",
    "\n",
    "For each optimization method, we report three quantities relative to the cubic\n",
    "objective’s stable minimizer at $x^\\star = 1$:\n",
    "\n",
    "- **Final gap:**  \n",
    "  $$| f(x_T) - f(x^\\star) |$$\n",
    "\n",
    "- **Best gap:**  \n",
    "  $$\\min_{t \\le T} \\; |f(x_t) - f(x^\\star)| $$\n",
    "\n",
    "- **Steps-to-tolerance:**  \n",
    "  The first iteration \\(t\\) such that  \n",
    "  $$|f(x_t) - f(x^\\star)| < \\varepsilon, \\qquad \\varepsilon = 10^{-4}$$\n",
    "\n",
    "These metrics quantify the speed, stability, and noise behavior of each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb69baf-f9a5-4cfa-a196-82c32baaf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Metrics: Gaps and Steps-to-Tolerance\n",
    "# ============================================\n",
    "\n",
    "def compute_metrics(xs, objective, x_star=1.0, eps=EPS):\n",
    "    \"\"\"\n",
    "    Compute final gap, best gap, and steps-to-tolerance.\n",
    "    Includes divergence detection via NaN check.\n",
    "    \"\"\"\n",
    "    xs = np.asarray(xs)\n",
    "\n",
    "    # Divergence / instability detection\n",
    "    if np.isnan(xs).any():\n",
    "        return np.nan, np.nan, None\n",
    "\n",
    "    gaps = np.abs(objective(xs) - objective(x_star))\n",
    "\n",
    "    final_gap = gaps[-1]\n",
    "    best_gap = gaps.min()\n",
    "\n",
    "    idx = np.where(gaps < eps)[0]\n",
    "    steps_to_tol = int(idx[0]) if len(idx) > 0 else None\n",
    "\n",
    "    return final_gap, best_gap, steps_to_tol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d37c31-af55-436d-a508-8a40b318f1cd",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400428f-0eae-4974-8035-d2d4efd008b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Run trajectories -----\n",
    "# GD\n",
    "xs_gd = gd_cubic(0.5, ETA_CUBIC)\n",
    "\n",
    "# SGD constant\n",
    "xs_sgd_const = sgd_constant(0.5, eta=ETA_SGD, sigma=SIGMA_SGD)\n",
    "\n",
    "# SGD diminishing\n",
    "xs_sgd_dim = sgd_diminishing(0.5, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD)\n",
    "\n",
    "# ----- Compute metrics -----\n",
    "\n",
    "gd_metrics     = compute_metrics(xs_gd, f)\n",
    "sgd_c_metrics  = compute_metrics(xs_sgd_const, f)\n",
    "sgd_d_metrics  = compute_metrics(xs_sgd_dim, f)\n",
    "\n",
    "# ----- Display -----\n",
    "print(\"=== Convergence Metrics (Cubic Objective) ===\")\n",
    "print(f\"GD (eta={ETA_CUBIC}):\")\n",
    "print(f\"  Final gap        = {gd_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {gd_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {gd_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Constant Step:\")\n",
    "print(f\"  Final gap        = {sgd_c_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_c_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_c_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Diminishing Step:\")\n",
    "print(f\"  Final gap        = {sgd_d_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_d_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_d_metrics[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4a564-c8c0-43b9-809f-ddedd8f3c8d6",
   "metadata": {},
   "source": [
    "## 8. Final Commentary — Key Takeaways\n",
    "\n",
    "### Gradient Descent (GD)\n",
    "\n",
    "- Deterministic update rule:\n",
    "  $$\n",
    "  x_{t+1} = x_t - \\eta f'(x_t)\n",
    "  $$\n",
    "- Converges cleanly and rapidly to the stable minimizer \\(x^\\star = 1\\).\n",
    "- Serves as a model of *idealized batch optimization* with no noise.\n",
    "\n",
    "### Stochastic Gradient Descent — Constant Step\n",
    "\n",
    "- Uses noisy gradients:\n",
    "  $$\n",
    "  g_t = f'(x_t) + \\varepsilon_t, \n",
    "  \\qquad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "  $$\n",
    "- Does **not** converge to the exact minimizer, but to a *noise ball* around it.\n",
    "- Steady-state radius scales as:\n",
    "  $$\n",
    "  \\text{radius} \\;\\propto\\; \\sqrt{\\eta}\\,\\sigma.\n",
    "  $$\n",
    "- Mimics **stochastic training behavior** seen in deep learning.\n",
    "\n",
    "### Stochastic Gradient Descent — Diminishing Step\n",
    "\n",
    "- Decaying schedule:\n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta_0}{1 + k t}\n",
    "  $$\n",
    "- Shrinks noise gradually, tightening convergence around $x^\\star$.\n",
    "- Behaves like **annealing**, **fine-tuning**, or **long-run stabilizing schedules** used in modern training pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
