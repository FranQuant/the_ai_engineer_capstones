{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90570b49-8522-4e37-b4d2-83b4ed754853",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week01_gd_optimization/gd_capstone.ipynb\n",
    ")\n",
    "\n",
    "# Gradient Descent Optimization — Week 1 Capstone\n",
    "\n",
    ">This notebook implements the Week-1 capstone for *The AI Engineer* program using two one-dimensional objectives:\n",
    "\n",
    "\n",
    "1. **Quadratic baseline**\n",
    "   $$\n",
    "   q(x) = \\tfrac{1}{2} x^2\n",
    "   $$\n",
    "   A simple convex function used to study gradient descent (GD) stability and step-size effects.\n",
    "\n",
    "2. **Simple cubic loss**\n",
    "   $$\n",
    "   f(x) = x^3 - 3x\n",
    "   $$\n",
    "   A function with multiple stationary points, illustrating basins of attraction and GD/SGD dynamics.\n",
    "\n",
    "We focus on:\n",
    "\n",
    ">- Deterministic **Gradient Descent (GD)**\n",
    ">- **Stochastic Gradient Descent (SGD)** with:\n",
    ">  - Constant step-size\n",
    ">  - Diminishing schedule $\\eta_t = \\dfrac{\\eta_0}{1 + k t}$\n",
    ">- **Step-size sensitivity** and stability\n",
    ">- **Reproducible experiments** via a single global seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2459a-8269-406e-86d9-72ff9fd7ff7a",
   "metadata": {},
   "source": [
    "## 0. Setup & Hyperparameters\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Component | Setting | Value / Formula |\n",
    "|----------|---------|------------------|\n",
    "| **Global seed** | Fixed integer | $\\text{SEED} = 123$ |\n",
    "| **GD step-sizes (quadratic)** | Sweep | $\\eta \\in \\{0.05,\\;0.10,\\;0.15,\\;0.20\\}$ |\n",
    "| **GD step-size (cubic)** | Fixed | $\\eta_{\\text{cubic}} = 0.05$ |\n",
    "| **SGD (constant)** | Step-size | $\\eta = \\eta_0 = 0.05$ |\n",
    "| **SGD (diminishing)** | Schedule | $\\eta_t = \\dfrac{\\eta_0}{1 + k t}$ |\n",
    "| **SGD decay constant** | $k$ | $k = 0.01$ |\n",
    "| **Tolerance** | Objective-gap | $\\varepsilon = 10^{-4}$ |\n",
    "| **Max iterations** | Upper bound | $T_{\\max} = 5000$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005faf11-d659-4fbc-85ed-df104c8a5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Setup & Hyperparameters\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Reproducible Global Seed ---\n",
    "SEED = 123\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# --- Learning Rates ---\n",
    "# Quadratic baseline sweep (for stability analysis)\n",
    "ETA_QUAD_SWEEP = [0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "# GD on the cubic objective\n",
    "ETA_CUBIC = 0.05\n",
    "\n",
    "# --- SGD Settings ---\n",
    "ETA0 = 0.05            # constant-step SGD (η)\n",
    "ETA_SGD = ETA0         # alias for clarity inside SGD functions\n",
    "K_SCHEDULE = 0.01      # diminishing-step decay factor k\n",
    "SIGMA_SGD = 0.5        # Gaussian noise scale σ\n",
    "\n",
    "# --- Global Experiment Settings ---\n",
    "EPS = 1e-4             # convergence tolerance on objective gap\n",
    "T_MAX = 5000           # iteration cap for GD/SGD\n",
    "\n",
    "# --- Plot Styling ---\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e9e73-3ad3-4df8-a5b3-d2cd1ca1dac3",
   "metadata": {},
   "source": [
    "## 1. Quadratic Baseline: Objective & Derivative\n",
    "\n",
    "We begin with the convex quadratic\n",
    "\n",
    "$$\n",
    "q(x) = \\tfrac{1}{2} x^2,\n",
    "$$\n",
    "\n",
    "which has a simple derivative\n",
    "\n",
    "$$\n",
    "q'(x) = x.\n",
    "$$\n",
    "\n",
    "This function is ideal for illustrating:\n",
    "\n",
    ">- **Stability** vs **instability** under different step sizes  \n",
    ">- Exact relationship between curvature and step-size limits  \n",
    ">- How GD behaves in a perfectly smooth, convex landscape\n",
    "\n",
    "The unique minimizer is\n",
    "\n",
    "$$\n",
    "x^\\star = 0.\n",
    "$$\n",
    "\n",
    "> Before running gradient descent, we implement the objective, derivative, and a helper function to compute the objective gap\n",
    "\n",
    "$$\n",
    "\\text{gap}(x_t) = q(x_t) - q(x^\\star).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c320520-8b34-4c36-b6a2-506233709d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Quadratic Baseline: Objective & Derivative\n",
    "# ============================================\n",
    "\n",
    "def q(x):\n",
    "    \"\"\"Quadratic objective: q(x) = 0.5 * x^2\"\"\"\n",
    "    return 0.5 * x**2\n",
    "\n",
    "def dq(x):\n",
    "    \"\"\"Derivative of the quadratic: q'(x) = x\"\"\"\n",
    "    return x\n",
    "\n",
    "def quad_gap(x):\n",
    "    \"\"\"Objective gap relative to optimum x* = 0.\"\"\"\n",
    "    return q(x) - q(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2f83-d278-43e1-ad44-89ffce40d33f",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent on the Quadratic — Step-Size Sweep\n",
    "\n",
    ">For the quadratic\n",
    ">$$\n",
    ">q(x) = \\tfrac{1}{2}x^2,\n",
    ">$$\n",
    ">Gradient Descent follows the update rule\n",
    ">$$\n",
    ">x_{t+1} = x_t - \\eta\\, q'(x_t) = x_t - \\eta x_t = (1 - \\eta)x_t.\n",
    ">$$\n",
    ">\n",
    ">This yields an exact stability condition:\n",
    ">$$\n",
    ">|1 - \\eta| < 1 \\quad \\Longleftrightarrow \\quad 0 < \\eta < 2.\n",
    ">$$\n",
    "\n",
    "A step-size sweep is ideal to illustrate:\n",
    "\n",
    ">- **Slow convergence** for small $\\eta$\n",
    ">- **Fast convergence** near the optimal $\\eta = 1$\n",
    ">- **Oscillations** for $1 < \\eta < 2$\n",
    ">- **Divergence** for $\\eta \\ge 2$\n",
    "\n",
    ">We run GD from the same initialization\n",
    ">$$\n",
    ">x_0 = 4,\n",
    ">$$\n",
    ">and compare trajectories for:\n",
    ">$$\n",
    ">\\eta \\in \\{0.05,\\; 0.10,\\; 0.15,\\; 0.20\\}.\n",
    ">$$\n",
    "\n",
    "A plot overlays these trajectories to show how step-size controls convergence speed and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7897db1-9d38-4428-a734-2a5fcfe4d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. GD on Quadratic — Step-Size Sweep\n",
    "# ============================================\n",
    "\n",
    "def gd_quadratic(x0, eta, T=T_MAX):\n",
    "    \"\"\"Runs GD on the quadratic for T steps.\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for _ in range(T):\n",
    "        x = x - eta * dq(x)   # dq(x) = x\n",
    "        xs.append(x)\n",
    "        if abs(x) < EPS:\n",
    "            break\n",
    "    return np.array(xs)\n",
    "\n",
    "# Initial point\n",
    "x0_quad = 4.0\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for eta in ETA_QUAD_SWEEP:\n",
    "    xs = gd_quadratic(x0_quad, eta)\n",
    "    plt.plot(xs, label=f\"η = {eta}\")\n",
    "\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.title(\"Quadratic GD — Step-Size Sweep\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.1,\n",
    "    \"GD trajectories for different step sizes on the quadratic objective.\\n\"\n",
    "    \"Small η converges slowly, η≈0.1 is faster, larger η approaches instability.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb9da5-fc87-410d-ad76-19816020a4b0",
   "metadata": {},
   "source": [
    "### Quadratic GD — Convergence Metrics\n",
    "\n",
    ">We compute simple convergence diagnostics for the quadratic baseline.\n",
    ">\n",
    ">- **Final gap**  \n",
    ">  $$\n",
    ">  |q(x_T) - q(0)| \n",
    ">  $$\n",
    ">\n",
    ">- **Best gap**  \n",
    ">  $$\n",
    ">  \\min_{t \\le T} |q(x_t) - q(0)|\n",
    ">  $$\n",
    ">\n",
    ">- **Steps-to-tolerance**  \n",
    ">  $$\n",
    ">  |q(x_t) - q(0)| < 10^{-4}\n",
    ">  $$\n",
    ">\n",
    ">For illustration, quadratic GD with $\\eta = 0.05$ (and in fact any $\\eta \\in \\{0.05, 0.10, 0.15, >0.20\\}$) is stable and converges rapidly on $q(x) = \\tfrac{1}{2}x^2$, exhibiting monotone decay >of the objective gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4853c9-d310-4505-ac19-85b33bbe199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Quadratic Metrics\n",
    "# ============================================\n",
    "\n",
    "def compute_quad_metrics(xs, eps=EPS):\n",
    "    \"\"\"Compute final gap, best gap, and steps-to-tolerance for q(x).\"\"\"\n",
    "    gaps = np.abs(q(xs) - q(0.0))\n",
    "\n",
    "    final_gap = gaps[-1]\n",
    "    best_gap = gaps.min()\n",
    "\n",
    "    idx = np.where(gaps < eps)[0]\n",
    "    steps_to_tol = int(idx[0]) if len(idx) > 0 else None\n",
    "\n",
    "    return final_gap, best_gap, steps_to_tol\n",
    "\n",
    "\n",
    "# Run GD on the quadratic with a representative step size\n",
    "xs_quad_metrics = gd_quadratic(4.0, eta=0.05)\n",
    "\n",
    "quad_metrics = compute_quad_metrics(xs_quad_metrics)\n",
    "\n",
    "print(\"=== Convergence Metrics (Quadratic Objective) ===\")\n",
    "print(f\"GD (eta=0.05):\")\n",
    "print(f\"  Final gap        = {quad_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {quad_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {quad_metrics[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457781a-0c3d-4a00-9864-9b21cb598530",
   "metadata": {},
   "source": [
    "## 3. Cubic Objective: Non-Convex Landscape & Derivative\n",
    "\n",
    "We now introduce the non-convex cubic function\n",
    "\n",
    ">$$\n",
    ">f(x) = x^3 - 3x,\n",
    ">$$\n",
    ">\n",
    ">which has derivative\n",
    ">\n",
    ">$$\n",
    ">f'(x) = 3x^2 - 3.\n",
    ">$$\n",
    "\n",
    "### Stationary points\n",
    "\n",
    ">Set $f'(x) = 0$:\n",
    ">\n",
    ">$$\n",
    "3x^2 - 3 = 0 \\quad \\Longrightarrow \\quad x = \\pm 1.\n",
    ">$$\n",
    ">\n",
    ">- **$x = -1$** is a **local maximum** (unstable under GD).  \n",
    ">- **$x = +1$** is a **local minimum** (stable for $0 < \\eta < \\tfrac{1}{3}$).\n",
    "\n",
    "### Basins of attraction\n",
    "\n",
    ">The cubic has only **one attracting basin** for positive learning rates:\n",
    ">\n",
    ">- If the initialization satisfies  $x_0 > -1$  \n",
    "  then Gradient Descent converges to the stable minimizer at $x=1$.\n",
    ">\n",
    ">- If $x_0 < -1$,  \n",
    "  gradient descent diverges to $-\\infty$ unless a guard prevents it.\n",
    ">\n",
    ">- The point $x=-1$ is a **measure-zero unstable fixed point**.\n",
    "\n",
    "**This makes the cubic ideal for illustrating:**\n",
    "\n",
    ">- **Stability vs. instability** of stationary points  \n",
    ">- **Basin structure** in non-convex problems  \n",
    ">- **Sensitivity to step size**  \n",
    ">- Differences between GD and noisy **SGD**\n",
    "  \n",
    "We now implement the objective, derivative, and gap function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b6903-9e33-4073-a2a6-de2c06f2dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Cubic Objective: f(x) = x^3 - 3x\n",
    "# ============================================\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Cubic objective: f(x) = x^3 - 3x\"\"\"\n",
    "    return x**3 - 3*x\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 3x^2 - 3\"\"\"\n",
    "    return 3*x**2 - 3\n",
    "\n",
    "def cubic_gap(x):\n",
    "    \"\"\"Objective gap relative to the stable minimizer x* = +1.\"\"\"\n",
    "    return f(x) - f(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5dfd9-e113-4242-9d5c-c8885d612f4e",
   "metadata": {},
   "source": [
    "### Cubic Objective — Function and Derivative\n",
    "*We now visualize the cubic landscape to understand the non-convex geometry before running optimization.*\n",
    "Before analyzing GD and SGD behavior on the cubic loss, we inspect the structure of the\n",
    "objective and its gradient.\n",
    "\n",
    ">The objective is:\n",
    ">\n",
    ">$$\n",
    ">f(x) = x^3 - 3x\n",
    ">$$\n",
    ">\n",
    ">and the derivative:\n",
    ">\n",
    ">$$\n",
    ">f'(x) = 3x^2 - 3.\n",
    ">$$\n",
    ">\n",
    ">The cubic landscape has a **local maximum at \\(x=-1\\)** and a **stable minimizer at \\(x=1\\)**.\n",
    ">These stationary points dictate the basin geometry that GD and SGD must navigate.\n",
    "\n",
    "The plots below visualize:\n",
    "- the function $f(x)$ over a moderate domain,\n",
    "- the derivative $f'(x)$,\n",
    "- and the curvature sign via the regions where $f'(x)$ changes fastest.\n",
    "\n",
    "These figures help interpret why GD diverges from certain initializations and why SGD\n",
    "stabilizes differently depending on step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc2e05-f52b-4bec-8f5a-8c673c5af1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cubic Diagnostics: f(x) and f'(x)\n",
    "# ============================================\n",
    "\n",
    "xs = np.linspace(-3, 3, 400)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# --- f(x) plot ---\n",
    "ax[0].plot(xs, f(xs), linewidth=2)\n",
    "ax[0].axvline(-1, linestyle=\"--\", color=\"grey\", label=\"x = -1 (max)\")\n",
    "ax[0].axvline(1, linestyle=\"--\", color=\"black\", label=\"x = 1 (min)\")\n",
    "ax[0].set_title(\"Cubic Objective  $f(x) = x^3 - 3x$\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"f(x)\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- f'(x) plot ---\n",
    "ax[1].plot(xs, df(xs), linewidth=2, color=\"darkorange\")\n",
    "ax[1].axhline(0, color=\"black\", linewidth=1)\n",
    "ax[1].axvline(-1, linestyle=\"--\", color=\"grey\")\n",
    "ax[1].axvline(1, linestyle=\"--\", color=\"black\")\n",
    "ax[1].set_title(\"Derivative  $f'(x) = 3x^2 - 3$\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"f'(x)\")\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.05,\n",
    "    \"The cubic has a saddle-like region around x=-1 and strong curvature near x=±∞.\\n\"\n",
    "    \"This explains sensitivity of GD to initialization and the steady-state variance of SGD.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89618-1b82-4243-a356-0b7ab7848d36",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent on the Cubic — Trajectories & Basins\n",
    "\n",
    "For the cubic\n",
    "\n",
    ">$$\n",
    ">f(x) = x^3 - 3x,\n",
    ">\\quad\n",
    ">f'(x) = 3x^2 - 3,\n",
    ">$$\n",
    ">\n",
    ">Gradient Descent follows the update rule\n",
    ">\n",
    ">$$\n",
    ">x_{t+1} = x_t - \\eta f'(x_t).\n",
    ">$$\n",
    ">\n",
    ">There are two stationary points:\n",
    ">\n",
    ">- **$x = -1$** — local maximum (repelling under GD)  \n",
    ">- **$x = +1$** — local minimum (attracting when $0 < \\eta < 1/3$)\n",
    "\n",
    "### Basin structure\n",
    "\n",
    "The cubic has a **single attracting basin**:\n",
    "\n",
    ">- If $x_0 > -1$\n",
    ">\n",
    ">- GD converges to $ x^\\star = 1 $.\n",
    ">\n",
    ">- If $x_0 < -1,$\n",
    ">\n",
    ">  GD diverges to $ -\\infty $ unless a guard prevents it.\n",
    "\n",
    "We run GD from several initial points to visualize this behavior:\n",
    "\n",
    ">$$\n",
    ">x_0 \\in \\{-3,\\; -0.5,\\; 0.5,\\; 2\\}.\n",
    ">$$\n",
    ">\n",
    ">We use the fixed step-size:\n",
    ">\n",
    ">$$\n",
    "<\\eta_{\\text{cubic}} = 0.05.\n",
    ">$$\n",
    "\n",
    "The resulting trajectories reveal the basin boundary at $x = -1$, the stable attractor at $x = 1$, and divergence when starting too far left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2453fd-60d7-4818-b618-ef89a3535e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. GD on the Cubic — Multiple Initializations\n",
    "# ============================================\n",
    "\n",
    "def gd_cubic(x0, eta=ETA_CUBIC, T=T_MAX):\n",
    "    \"\"\"Runs GD on the cubic for T steps (with guard for divergence).\"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for _ in range(T):\n",
    "        x = x - eta * df(x)\n",
    "\n",
    "        # Divergence guard to avoid blowing up the plot\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "# Initializations to test\n",
    "inits = [-3.0, -0.5, 0.5, 2.0]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for x0 in inits:\n",
    "    xs = gd_cubic(x0, ETA_CUBIC)\n",
    "    plt.plot(xs[:20], label=f\"x0 = {x0}\")\n",
    "\n",
    "plt.axhline(1.0, color=\"black\", linewidth=1, linestyle=\"--\", label=\"x* = 1\")\n",
    "plt.title(\"GD on the Cubic — Basin Behavior\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.1,\n",
    "    \"GD trajectories on the cubic. Initializations right of -1 converge to x*=1; \"\n",
    "    \"starting left of -1 diverges without guards.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4eeeec-c780-4441-a71b-6f3e60f4bd09",
   "metadata": {},
   "source": [
    "## 5. Stochastic Gradient Descent (SGD) on the Cubic\n",
    "\n",
    "Stochastic Gradient Descent introduces noise into the gradient evaluation.  \n",
    "Instead of using the exact derivative  \n",
    ">$$\n",
    ">f'(x) = 3x^2 - 3,\n",
    "<$$  \n",
    ">SGD uses a noisy estimate:\n",
    ">$$\n",
    ">g_t = f'(x_t) + \\varepsilon_t, \n",
    ">\\qquad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2).\n",
    ">$$\n",
    ">\n",
    ">This produces characteristic **wandering** around the stable minimizer $x^\\star = 1$, with a  \n",
    ">steady-state radius proportional to $$\\mathrm{Var}(x_t) \\;\\propto\\; \\eta\\,\\sigma^2$$\n",
    "\n",
    "We implement two SGD schedules:\n",
    "\n",
    ">1. **Constant step-size**\n",
    ">   $$\n",
    ">   x_{t+1} = x_t - \\eta \\, g_t\n",
    ">   $$\n",
    ">   This converges only to a *neighborhood* of $x^\\star$.\n",
    ">\n",
    ">2. **Diminishing step-size**\n",
    ">   $$\n",
    ">   \\eta_t = \\frac{\\eta_0}{1 + k t}\n",
    ">   $$\n",
    ">   $$\n",
    ">   x_{t+1} = x_t - \\eta_t \\, g_t\n",
    ">   $$\n",
    ">   This gradually shrinks noise and yields a tighter neighborhood around the minimizer.\n",
    "\n",
    "We now define both SGD variants using the **shared global RNG** for determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136beb3-d680-4ee3-8c6c-2595e7a4eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. SGD on the Cubic — Constant & Diminishing Step (Shared RNG Version)\n",
    "# ============================================\n",
    "\n",
    "def sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_MAX, rng=rng):\n",
    "    \"\"\"\n",
    "    SGD with constant step size:\n",
    "        x_{t+1} = x_t - eta * (df(x_t) + noise)\n",
    "    Uses the shared global RNG for reproducibility and consistent noise paths.\n",
    "    \"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for _ in range(T):\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta * (df(x) + noise)\n",
    "\n",
    "        # Divergence guard\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "def sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_MAX, rng=rng):\n",
    "    \"\"\"\n",
    "    SGD with diminishing step size:\n",
    "        eta_t = eta0 / (1 + k t)\n",
    "    Uses the shared global RNG for reproducibility and consistent noise paths.\n",
    "    \"\"\"\n",
    "    xs = [x0]\n",
    "    x = x0\n",
    "    for t in range(T):\n",
    "        eta_t = eta0 / (1.0 + k * t)\n",
    "        noise = rng.normal(0, sigma)\n",
    "        x = x - eta_t * (df(x) + noise)\n",
    "\n",
    "        # Divergence guard\n",
    "        if abs(x) > 20:\n",
    "            xs.append(np.nan)\n",
    "            break\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    return np.array(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a393af0-84f2-49ae-aa62-d9646ee24727",
   "metadata": {},
   "source": [
    "## 6. SGD Trajectories on the Cubic\n",
    "\n",
    "We now compare **constant-step** and **diminishing-step** SGD on the cubic objective  \n",
    "starting from the same initialization $x_0 = 0.5$.\n",
    "\n",
    ">- With a **constant step**, SGD converges only to a *noisy neighborhood* around  \n",
    ">  $ x^\\star = 1 $, with fluctuations scaling like $$\\text{radius} \\;\\propto\\; \\eta^{1/2}\\,\\sigma $$\n",
    ">\n",
    ">- With a **diminishing schedule**  \n",
    ">  \n",
    ">  $$ \\eta_t = \\frac{\\eta_0}{1 + k t} $$\n",
    "> \n",
    ">  the noise gradually shrinks, resulting in a tighter approach to the minimizer.\n",
    "\n",
    "The plot below compares both trajectories over $ T = 200 $ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8fb9a-3b2b-4fa5-8dd6-a0ce822fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. SGD Trajectories — Constant vs Diminishing\n",
    "# ============================================\n",
    "\n",
    "T_plot = 200\n",
    "x0 = 0.5\n",
    "\n",
    "xs_const = sgd_constant(x0, eta=ETA_SGD, sigma=SIGMA_SGD, T=T_plot)\n",
    "xs_dimin = sgd_diminishing(x0, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=T_plot)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(xs_const, label=\"Constant SGD\", linewidth=2)\n",
    "plt.plot(xs_dimin, label=\"Diminishing-step SGD\", linewidth=2)\n",
    "plt.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"x* = 1\")\n",
    "\n",
    "plt.title(\"SGD on the Cubic — Constant vs Diminishing Step\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x_t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figtext(\n",
    "    0.5, -0.12,\n",
    "    \"Constant-step SGD fluctuates around x*=1; diminishing-step SGD converges tighter as eta_t decreases.\",\n",
    "    ha=\"center\", fontsize=9\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380ea1d-98a2-4b26-8a53-9fa24046aed0",
   "metadata": {},
   "source": [
    "## 7. Metrics: Final Gap, Best Gap, and Steps-to-Tolerance\n",
    "\n",
    "For each optimization method, we report three quantities relative to the cubic\n",
    "objective’s stable minimizer at $x^\\star = 1$:\n",
    "\n",
    ">- **Final gap:**  \n",
    ">  $$\n",
    ">  | f(x_T) - f(x^\\star) |\n",
    ">  $$\n",
    ">\n",
    ">- **Best gap:**  \n",
    ">  $$\n",
    ">  \\min_{t \\le T} \\; |f(x_t) - f(x^\\star)| \n",
    ">  $$\n",
    ">\n",
    ">- **Steps-to-tolerance:**  \n",
    ">  The first iteration \\(t\\) such that  \n",
    ">  $$\n",
    ">  |f(x_t) - f(x^\\star)| < \\varepsilon, \\qquad \\varepsilon = 10^{-4}.\n",
    ">  $$\n",
    "\n",
    "These metrics quantify convergence speed and stability for GD, constant-step SGD,\n",
    "and diminishing-step SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb69baf-f9a5-4cfa-a196-82c32baaf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Metrics: Gaps and Steps-to-Tolerance\n",
    "# ============================================\n",
    "\n",
    "def compute_metrics(xs, objective, x_star=1.0, eps=EPS):\n",
    "    \"\"\"Compute final gap, best gap, and steps-to-tolerance.\"\"\"\n",
    "    gaps = np.abs(objective(xs) - objective(x_star))\n",
    "\n",
    "    final_gap = gaps[-1]\n",
    "    best_gap = gaps.min()\n",
    "\n",
    "    # First iteration where gap < eps (if any)\n",
    "    idx = np.where(gaps < eps)[0]\n",
    "    steps_to_tol = int(idx[0]) if len(idx) > 0 else None\n",
    "\n",
    "    return final_gap, best_gap, steps_to_tol\n",
    "\n",
    "\n",
    "# ----- Run trajectories -----\n",
    "\n",
    "# GD\n",
    "xs_gd = gd_cubic(0.5, ETA_CUBIC)\n",
    "\n",
    "# SGD constant\n",
    "xs_sgd_const = sgd_constant(0.5, eta=ETA_SGD, sigma=SIGMA_SGD)\n",
    "\n",
    "# SGD diminishing\n",
    "xs_sgd_dim = sgd_diminishing(0.5, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD)\n",
    "\n",
    "\n",
    "# ----- Compute metrics -----\n",
    "\n",
    "gd_metrics     = compute_metrics(xs_gd, f)\n",
    "sgd_c_metrics  = compute_metrics(xs_sgd_const, f)\n",
    "sgd_d_metrics  = compute_metrics(xs_sgd_dim, f)\n",
    "\n",
    "\n",
    "# ----- Display -----\n",
    "\n",
    "print(\"=== Convergence Metrics (Cubic Objective) ===\")\n",
    "print(f\"GD (eta={ETA_CUBIC}):\")\n",
    "print(f\"  Final gap        = {gd_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {gd_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {gd_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Constant Step:\")\n",
    "print(f\"  Final gap        = {sgd_c_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_c_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_c_metrics[2]}\")\n",
    "\n",
    "print(\"\\nSGD Diminishing Step:\")\n",
    "print(f\"  Final gap        = {sgd_d_metrics[0]:.6f}\")\n",
    "print(f\"  Best gap         = {sgd_d_metrics[1]:.6f}\")\n",
    "print(f\"  Steps-to-tol     = {sgd_d_metrics[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf44ea2-830a-40a8-a1c3-63b5ef6ff64f",
   "metadata": {},
   "source": [
    "## 8. Runtime Audit — End-to-End Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8650f0-3dce-4971-bcc6-698da199298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. Runtime Audit — End-to-End Timing\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "\n",
    "def run_full_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the core GD/SGD routines once to measure total runtime.\n",
    "    Keeps everything lightweight and deterministic.\n",
    "    \"\"\"\n",
    "    # Quadratic GD\n",
    "    _ = gd_quadratic(4.0, eta=0.05)\n",
    "\n",
    "    # Cubic GD\n",
    "    _ = gd_cubic(0.5, eta=ETA_CUBIC)\n",
    "\n",
    "    # SGD constant & diminishing\n",
    "    _ = sgd_constant(0.5, eta=ETA_SGD, sigma=SIGMA_SGD, T=300)\n",
    "    _ = sgd_diminishing(0.5, eta0=ETA0, k=K_SCHEDULE, sigma=SIGMA_SGD, T=300)\n",
    "\n",
    "    # Metrics\n",
    "    _ = compute_metrics(_, f)\n",
    "\n",
    "# Measure runtime\n",
    "t0 = time.time()\n",
    "run_full_pipeline()\n",
    "t1 = time.time()\n",
    "\n",
    "elapsed = t1 - t0\n",
    "print(\"=== Runtime Audit ===\")\n",
    "print(f\"Total execution time: {elapsed:.4f} seconds\")\n",
    "print(\"\\n(Note: Expected target is ~2 seconds or less on Colab/CPU.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4a564-c8c0-43b9-809f-ddedd8f3c8d6",
   "metadata": {},
   "source": [
    "## 8. Final Commentary — Key Takeaways\n",
    "\n",
    "- **GD** converges rapidly and deterministically to the stable minimizer  \n",
    "  $$x^\\star = 1,$$  \n",
    "  reaching tolerance in ~14 steps for $\\eta=0.05$.\n",
    "\n",
    "- **SGD (constant step)** uses  \n",
    "  $$g_t = f'(x_t) + \\varepsilon_t,$$  \n",
    "  and settles into a stationary **noise ball** around $x^\\star$ with radius  \n",
    "  $$\\propto \\sigma.$$\n",
    "\n",
    "- **SGD (diminishing step)** with  \n",
    "  $$\\eta_t = \\frac{\\eta_0}{1 + k t}$$  \n",
    "  contracts noise over time and achieves slow but genuine convergence.\n",
    "\n",
    "- **Tradeoffs:**  \n",
    "  GD = fast & stable;  \n",
    "  Constant-SGD = exploratory but noisy;  \n",
    "  Diminishing-SGD = convergent but slow.\n",
    "\n",
    "These dynamics mirror the core behaviors of optimization in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
