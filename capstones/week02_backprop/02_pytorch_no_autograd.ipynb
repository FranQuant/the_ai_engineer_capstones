{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397447dd-88ba-4d25-81fb-bfaba757c7cf",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/week02_backprop/02_pytorch_no_autograd.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce788974-bae7-48f4-9817-1ee550ef8459",
   "metadata": {},
   "source": [
    "# 02 — PyTorch Implementation (No Autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ee607-da99-415c-b6ca-5b4d4f37f73a",
   "metadata": {},
   "source": [
    "## 1. Imports & Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a134e7-b3d7-4fff-8332-2a0d14bdfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Imports & Deterministic Seeds (match NB01)\n",
    "# ============================================\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392240b-ccf5-43c5-8555-8a2dd50f1316",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset (same as Notebook 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc0422-762c-4391-942b-9fe757c6ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Synthetic Dataset (match NB01 exactly)\n",
    "# ============================================\n",
    "N = 500  # same as Notebook 01\n",
    "\n",
    "X_np = rng.uniform(-1, 1, size=(N, 2)).astype(np.float32)\n",
    "y_np = (X_np[:, 0] * X_np[:, 1] < 0).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch (float32 everywhere)\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3a172-21d1-4096-877e-14eca1a27144",
   "metadata": {},
   "source": [
    "## 3. NumPy Reference Forward Pass (from Notebook 01)\n",
    "To ensure numerical parity between the NumPy and PyTorch implementations,\n",
    "we replicate the minimal forward-pass functions from Notebook 01. These\n",
    "are used for direct comparison in Section 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253fc2b-2e73-474f-8cf2-aa9fba8062cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. NumPy Reference Forward Pass (from NB01)\n",
    "# ============================================\n",
    "\n",
    "def relu_np(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_single(x, W1, b1, W2, b2):\n",
    "    a1 = W1 @ x + b1        # (h,)\n",
    "    h  = relu_np(a1)        # (h,)\n",
    "    f  = W2 @ h + b2        # (1,)\n",
    "    return a1, h, float(f[0])  # explicit scalar extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f9bb4-49a9-406a-897d-bde7c9f292f3",
   "metadata": {},
   "source": [
    "## 4 .NumPy Model Parameters (for comparison)\n",
    "\n",
    "Notebook 02 needs standalone NumPy parameters to reproduce the exact\n",
    "forward pass used in Notebook 01. These are synchronized with the\n",
    "PyTorch parameters in Section 5 so both implementations produce\n",
    "identical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2fb68-9e2e-4a91-8186-f30284d2d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. NumPy Model Parameters (match NB01 exactly)\n",
    "# ============================================\n",
    "\n",
    "d, h = 2, 4\n",
    "\n",
    "# Weight initialization: small Gaussian (std = 0.1), cast to float32\n",
    "W1 = rng.normal(0.0, 0.1, size=(h, d)).astype(np.float32)\n",
    "W2 = rng.normal(0.0, 0.1, size=(1, h)).astype(np.float32)\n",
    "\n",
    "# Biases as float32\n",
    "b1 = np.zeros((h,), dtype=np.float32)\n",
    "b2 = np.zeros((1,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312d2b3-776b-4436-8e37-5ec76979071f",
   "metadata": {},
   "source": [
    "## 5. Model Parameters in PyTorch (No Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febf33a-c316-4b3e-88cd-0deaacfaebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. PyTorch Parameters (synced with NumPy)\n",
    "# ============================================\n",
    "\n",
    "# Create tensors with the same shapes and dtypes as NumPy params\n",
    "W1_t = torch.empty((h, d), dtype=torch.float32)\n",
    "b1_t = torch.empty((h,),   dtype=torch.float32)\n",
    "W2_t = torch.empty((1, h), dtype=torch.float32)\n",
    "b2_t = torch.empty((1,),   dtype=torch.float32)\n",
    "\n",
    "# Disable autograd for this notebook\n",
    "for t in [W1_t, b1_t, W2_t, b2_t]:\n",
    "    t.requires_grad_(False)\n",
    "\n",
    "# Sync PyTorch parameters with NumPy (safe in-place copy)\n",
    "W1_t.copy_(torch.tensor(W1, dtype=torch.float32))\n",
    "b1_t.copy_(torch.tensor(b1, dtype=torch.float32))\n",
    "W2_t.copy_(torch.tensor(W2, dtype=torch.float32))\n",
    "b2_t.copy_(torch.tensor(b2, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1b9d-f685-4c26-a749-3ad5cbdc7e7d",
   "metadata": {},
   "source": [
    "## 6. Activation Function\n",
    "#### Match NumPy ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711a7ed-e190-4553-95cf-dd87b74abb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_t(x):\n",
    "    return torch.clamp(x, min=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed0b5e-44c8-454b-bb1a-ada378e8c3f9",
   "metadata": {},
   "source": [
    "## 7. Forward Pass (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5e5a0-01eb-4cd7-9981-0fec9f791a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Forward Pass (PyTorch) — single-sample only\n",
    "# ============================================\n",
    "\n",
    "def forward_torch(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Single-sample forward pass matching NumPy forward_single.\n",
    "    x:  (d,)\n",
    "    W1: (h, d)\n",
    "    b1: (h,)\n",
    "    W2: (1, h)\n",
    "    b2: (1,)\n",
    "    Returns:\n",
    "        a1: pre-activation (h,)\n",
    "        h:  hidden layer (h,)\n",
    "        f:  scalar output (torch scalar tensor)\n",
    "    \"\"\"\n",
    "    a1 = x @ W1.T + b1        # (h,)\n",
    "    h  = relu_t(a1)\n",
    "    f  = W2 @ h + b2          # (1,)\n",
    "    return a1, h, f.squeeze()  # scalar tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b4c5b-1e6a-4b9f-ba11-acf6375a4199",
   "metadata": {},
   "source": [
    "## 8. Loss Function\n",
    "#### Match NumPy MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafde80-f281-4d57-aed2-8de231a950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. Loss Function (match NumPy exactly)\n",
    "# ============================================\n",
    "\n",
    "def mse_loss_t(f, y):\n",
    "    \"\"\"\n",
    "    Mean-squared error loss for a single sample.\n",
    "    Matches the NumPy definition L = 0.5 * (f - y)**2.\n",
    "    f: torch scalar tensor\n",
    "    y: torch scalar tensor (float32)\n",
    "    Returns: torch scalar tensor\n",
    "    \"\"\"\n",
    "    return 0.5 * (f - y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d6636-ed17-41ed-908f-2065b81396ee",
   "metadata": {},
   "source": [
    "## 9. Numerical Consistency Test (NumPy vs Torch)\n",
    "We compare the NumPy output from Notebook 01 with the PyTorch output here.\n",
    "\n",
    "Pick a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e4eca-5d37-4092-81e4-46a140c6fc52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. Numerical Consistency Test (NumPy vs Torch)\n",
    "# ============================================\n",
    "\n",
    "i = 0  # pick sample\n",
    "x_i_np = X_np[i]              # NumPy input\n",
    "y_i    = y[i]                 # Torch scalar tensor target\n",
    "\n",
    "# NumPy forward pass\n",
    "a1_np, h_np, f_np = forward_single(x_i_np, W1, b1, W2, b2)   # scalar f_np (Python float)\n",
    "\n",
    "# PyTorch forward pass\n",
    "x_i_t = X[i]                  # torch.float32\n",
    "a1_t, h_t, f_t = forward_torch(x_i_t, W1_t, b1_t, W2_t, b2_t)  # f_t is torch scalar tensor\n",
    "\n",
    "f_t_val = f_t.item()\n",
    "\n",
    "# Print comparison\n",
    "print(\"NumPy output f_np =\", f_np)\n",
    "print(\"Torch output f_t =\", f_t_val)\n",
    "print(\"Absolute difference =\", abs(f_np - f_t_val))\n",
    "\n",
    "# Assertion — ensures consistency\n",
    "assert abs(f_np - f_t_val) < 1e-6, \"NumPy and PyTorch outputs diverge!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ee6d8-1c76-4a24-9787-f327a4203792",
   "metadata": {},
   "source": [
    "## 10. Manual Backward (No Autograd)\n",
    "\n",
    "We now re-implement the **chain-rule gradients** from Notebook 01, but using\n",
    "PyTorch tensors with `requires_grad=False`. This keeps autograd disabled while\n",
    "showing how backprop works numerically in Torch.\n",
    "\n",
    "For a single sample $(x, y)$ and forward pass\n",
    "$f = W_2 \\, \\text{ReLU}(W_1 x + b_1) + b_2$ with loss\n",
    "$L = \\tfrac12 (f - y)^2$, the gradients are:\n",
    "\n",
    "- $dL/df = f - y$\n",
    "- $dL/dW_2 = (f - y)\\,h^\\top$\n",
    "- $dL/db_2 = f - y$\n",
    "- $dL/dh = W_2^\\top (f - y)$\n",
    "- $dL/da_1 = dL/dh \\odot \\mathbf{1}_{a_1 > 0}$\n",
    "- $dL/dW_1 = (dL/da_1)\\,x^\\top$\n",
    "- $dL/db_1 = dL/da_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251f206-e4db-4c16-8a7a-a802c9fccf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. Manual Backward (no autograd)\n",
    "# ============================================\n",
    "\n",
    "def backward_torch(x, y, a1, h, f, W1, W2):\n",
    "    \"\"\"\n",
    "    Manual chain-rule gradients in PyTorch (no autograd).\n",
    "    All inputs are torch tensors with requires_grad=False.\n",
    "    Shapes:\n",
    "        x:  (d,)\n",
    "        a1: (h,)\n",
    "        h:  (h,)\n",
    "        f:  scalar tensor\n",
    "        W1: (h, d)\n",
    "        W2: (1, h)\n",
    "        y:  scalar tensor\n",
    "    Returns:\n",
    "        dW1: (h, d)\n",
    "        db1: (h,)\n",
    "        dW2: (1, h)\n",
    "        db2: (1,)\n",
    "    \"\"\"\n",
    "    # dL/df\n",
    "    df = f - y                # scalar tensor\n",
    "\n",
    "    # Output layer\n",
    "    dW2 = df * h[None, :]     # (1, h)\n",
    "    db2 = df.unsqueeze(0)     # (1,)\n",
    "\n",
    "    # Hidden layer\n",
    "    dh  = W2[0] * df          # (h,)\n",
    "    da1 = dh * (a1 > 0).float()  # ReLU'\n",
    "\n",
    "    # Input layer\n",
    "    dW1 = da1[:, None] @ x[None, :]  # (h, d)\n",
    "    db1 = da1                        # (h,)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b39dd-4717-4697-9f89-539ad3772398",
   "metadata": {},
   "source": [
    "## 11. Tiny Training Loop (Manual SGD, No Autograd)\n",
    "\n",
    "We now run a small training loop using **manual gradients** only. Autograd\n",
    "remains disabled (`requires_grad=False`), and we update `W1_t, b1_t, W2_t, b2_t`\n",
    "in-place using SGD on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ebbf0-37dc-43da-8117-98b636d99dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 11. Tiny Training Loop (manual SGD, no autograd)\n",
    "# ============================================\n",
    "\n",
    "# Re-initialize Torch parameters from the NumPy baseline\n",
    "W1_t.copy_(torch.tensor(W1, dtype=torch.float32))\n",
    "b1_t.copy_(torch.tensor(b1, dtype=torch.float32))\n",
    "W2_t.copy_(torch.tensor(W2, dtype=torch.float32))\n",
    "b2_t.copy_(torch.tensor(b2, dtype=torch.float32))\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = X[i]       # (d,)\n",
    "        y_i = y[i]       # scalar tensor\n",
    "\n",
    "        # Forward\n",
    "        a1, h, f = forward_torch(x_i, W1_t, b1_t, W2_t, b2_t)  # f is scalar tensor\n",
    "        loss = mse_loss_t(f, y_i)\n",
    "\n",
    "        # Backward (manual)\n",
    "        dW1, db1, dW2, db2 = backward_torch(x_i, y_i, a1, h, f, W1_t, W2_t)\n",
    "\n",
    "        # SGD update (in-place)\n",
    "        W1_t -= learning_rate * dW1\n",
    "        b1_t -= learning_rate * db1\n",
    "        W2_t -= learning_rate * dW2\n",
    "        b2_t -= learning_rate * db2\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= N\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch:03d} | mean loss {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9e7c8-d060-47e0-b78f-0aa7d0d2a584",
   "metadata": {},
   "source": [
    "## 12. Accuracy After Manual-Gradient Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a7013-862e-48f9-af18-8b61958f3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 12. Accuracy after training\n",
    "# ============================================\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        _, _, f = forward_torch(x_i, W1_t, b1_t, W2_t, b2_t)\n",
    "        y_hat = (f >= 0.5).float()\n",
    "        correct += (y_hat == y_i).item()\n",
    "\n",
    "accuracy = correct / N\n",
    "print(f\"Final training accuracy (manual gradients): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b9ceb-cbe0-4732-ba00-d20637185ca5",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "In this notebook we re-implemented the 2-layer neural network forward pass in\n",
    "PyTorch **without autograd**, matching the NumPy reference model from Notebook 01\n",
    "exactly. To guarantee full numerical parity, we replicated:\n",
    "\n",
    "- the same dataset generation (same RNG, same sampling),\n",
    "- the same parameter initialization (Gaussian weights, zero biases),\n",
    "- the same forward equations (linear → ReLU → linear),\n",
    "- the same loss definition \\( L = \\tfrac12 (f - y)^2 \\),\n",
    "- the same float32 dtype end-to-end.\n",
    "\n",
    "After synchronization, NumPy and PyTorch forward passes matched to machine\n",
    "precision, confirming deterministic equivalence.\n",
    "\n",
    "We then extended this baseline with **manual backpropagation** implemented\n",
    "directly in PyTorch tensors (`requires_grad=False`) and ran a small **SGD\n",
    "training loop** using only our hand-derived gradients. The model trained\n",
    "successfully and achieved reasonable accuracy on the quadrant-classification\n",
    "task.\n",
    "\n",
    "This notebook establishes the foundation for Notebook 03, where we introduce\n",
    "**PyTorch autograd**, compare automatic gradients to our manual derivatives, and\n",
    "validate that the computational graph reproduces the exact algebra from\n",
    "Notebook 01."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
