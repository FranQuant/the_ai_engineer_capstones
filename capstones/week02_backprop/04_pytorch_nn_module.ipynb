{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269f2252-6c07-469b-9299-d391c86184c8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/Week02_backprop/04_pytorch_nn_module.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d31c3",
   "metadata": {},
   "source": [
    "# 04 — PyTorch nn.Module + DataLoader (1-Hidden-Layer MLP)\n",
    "\n",
    "Stage 4 of the Week-02 capstone: wrap the two-layer XOR MLP in `nn.Module`, keep the same data/initialization as 01–03, and train with mini-batch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc19378",
   "metadata": {},
   "source": [
    "## 1. Imports & Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# Deterministic seeds (match 01/02/03)\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    global rng\n",
    "    torch.manual_seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(\"Seeds set to\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86123905",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset (XOR logic)\n",
    "\n",
    "Matches Notebooks 01–03:\n",
    "$$x \\sim \text{Uniform}([-1,1]^2),\\; y = \\mathbf{1}[x_1 x_2 < 0].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc831a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy_data(n_samples=500):\n",
    "    X = rng.uniform(-1, 1, size=(n_samples, 2)).astype(np.float32)\n",
    "    y = (X[:, 0] * X[:, 1] < 0).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "X_np, y_np = generate_toy_data()\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape, \"dtype:\", X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58e254",
   "metadata": {},
   "source": [
    "## 3. nn.Module Parameters + Forward\n",
    "\n",
    "Manual parameters inside an `nn.Module`, mirroring $W_1, b_1, W_2, b_2$ from Notebooks 01–03:\n",
    "- $W_1: (h, d)$, $b_1: (h,)$\n",
    "- $W_2: (1, h)$, $b_2: (1,)$\n",
    "- Gaussian $\\mathcal{N}(0, 0.1)$ for weights, zeros for biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerXOR(torch.nn.Module):\n",
    "    def __init__(self, d=2, h=4, out=1):\n",
    "        super().__init__()\n",
    "        W1 = torch.tensor(rng.normal(0.0, 0.1, size=(h, d)), dtype=torch.float32)\n",
    "        b1 = torch.zeros(h, dtype=torch.float32)\n",
    "        W2 = torch.tensor(rng.normal(0.0, 0.1, size=(out, h)), dtype=torch.float32)\n",
    "        b2 = torch.zeros(out, dtype=torch.float32)\n",
    "        self.W1 = torch.nn.Parameter(W1)\n",
    "        self.b1 = torch.nn.Parameter(b1)\n",
    "        self.W2 = torch.nn.Parameter(W2)\n",
    "        self.b2 = torch.nn.Parameter(b2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a1 = x @ self.W1.T + self.b1   # (batch, h)\n",
    "        h = torch.relu(a1)             # (batch, h)\n",
    "        f = h @ self.W2.T + self.b2    # (batch, 1)\n",
    "        return f.squeeze(-1)           # (batch,)\n",
    "\n",
    "\n",
    "model = TwoLayerXOR(d=2, h=4, out=1)\n",
    "print(model)\n",
    "print({name: tuple(p.shape) for name, p in model.named_parameters()})\n",
    "\n",
    "with torch.no_grad():\n",
    "    f0 = model(X[:4])\n",
    "    print(\"Forward on 4 samples ->\", f0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991ec8a",
   "metadata": {},
   "source": [
    "## 4. DataLoader (mini-batching)\n",
    "\n",
    "Shuffles the full XOR dataset each epoch while preserving determinism via the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=gen)\n",
    "\n",
    "print(\"batches per epoch:\", len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a87110",
   "metadata": {},
   "source": [
    "## 5. Loss Function & Optimizer (SGD)\n",
    "\n",
    "Use mean-squared error (same target as prior notebooks) and plain SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ece6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534e5d3",
   "metadata": {},
   "source": [
    "## 6. Training Loop (mini-batch SGD + gradient norms)\n",
    "\n",
    "Log mean loss per epoch and the $\\ell_2$ norm of all parameter gradients (last batch of each epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a47bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "loss_history = []\n",
    "grad_norm_history = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        total_norm_sq = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm_sq += p.grad.norm().item() ** 2\n",
    "        grad_norm = total_norm_sq ** 0.5\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    mean_loss = epoch_loss / num_batches\n",
    "    loss_history.append(mean_loss)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"epoch {epoch:03d} | loss {mean_loss:.4f} | grad-norm {grad_norm:.4f}\")\n",
    "\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952dfdba",
   "metadata": {},
   "source": [
    "## 7. Loss Curve\n",
    "\n",
    "Mean loss per epoch over the mini-batch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history, label=\"MSE loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss (mini-batch SGD)\")\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7711ce5",
   "metadata": {},
   "source": [
    "## 8. Accuracy on Final Model\n",
    "\n",
    "Threshold the scalar output at 0.5 to compute XOR classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_full = model(X)\n",
    "    y_hat = (preds_full >= 0.5).float()\n",
    "    accuracy = (y_hat == y).float().mean().item()\n",
    "\n",
    "print(\"Final accuracy:\", round(accuracy, 4))\n",
    "print(\"Sample predictions:\", preds_full[:5].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd359a3",
   "metadata": {},
   "source": [
    "## 9. Gradient Norm Trace\n",
    "\n",
    "Inspect the recorded gradient norms from the last batch of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d36051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient norms (first 5):\", grad_norm_history[:5])\n",
    "print(\"Gradient norms (last 5):\", grad_norm_history[-5:])\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "plt.plot(grad_norm_history)\n",
    "plt.xlabel(\"Batch step\")\n",
    "plt.ylabel(\"Grad norm (L2)\")\n",
    "plt.title(\"Gradient Norms over Training\")\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828eb27",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "- Seeds, data distribution, and parameter init match Notebooks 01–03.\n",
    "- The `nn.Module` mirrors the manual parameters ($W_1, b_1, W_2, b_2$) and uses the same ReLU hidden layer.\n",
    "- Mini-batch SGD converges quickly; loss and gradient norms shrink steadily.\n",
    "- Final accuracy approaches 1.0, confirming the learned XOR mapping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
