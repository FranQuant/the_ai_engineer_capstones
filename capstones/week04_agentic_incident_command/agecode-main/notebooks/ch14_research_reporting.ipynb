{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc48740",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067f176",
   "metadata": {
    "id": "t"
   },
   "source": [
    "# AI Agents & Automation \u2014 Chapter 14\n",
    "## Case Study \u2014 Research & Reporting Assistant (RAG + Planning)\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch<br>\n",
    "AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43843605",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook accompanies Chapter 14 \u2014 Research Reporting. It is self-contained and demonstrates the core ideas with small, readable code cells. Run cells from top to bottom; each code cell is preceded by a short explanation of what it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad3378",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "This self-contained notebook shows a tiny retrieval\u2192plan\u2192draft\u2192fact-check flow using a local (dependency-free) retriever. Swap parts later for framework equivalents while keeping the same interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fb63a",
   "metadata": {
    "id": "retriever"
   },
   "outputs": [],
   "source": [
    "import math  # import math utilities\n",
    "import re  # import regex utilities\n",
    "\n",
    "def tokenize(text: str) -> list[str]:  # extract lowercase tokens\n",
    "    return re.findall(r'[A-Za-z0-9_]+', text.lower())  # split into tokens\n",
    "\n",
    "def bow(\n",
    "    text: str,\n",
    "    vocab: dict[str, int],\n",
    ") -> list[float]:\n",
    "    # build normalized bag-of-words vector\n",
    "    tokens = tokenize(text)  # collect tokens\n",
    "    for token in tokens:  # grow vocab when new terms arrive\n",
    "        vocab.setdefault(token, len(vocab))  # add new token\n",
    "    vec = [0.0] * len(vocab)  # allocate vector\n",
    "    for token in tokens:  # tally token frequency\n",
    "        vec[vocab[token]] += 1.0  # count tokens\n",
    "    norm = math.sqrt(sum(value * value for value in vec)) or 1.0  # compute norm\n",
    "    return [value / norm for value in vec]  # normalize\n",
    "\n",
    "def pad(\n",
    "    a: list[float],\n",
    "    b: list[float],\n",
    ") -> tuple[list[float], list[float]]:\n",
    "    # align vector lengths for cosine\n",
    "    size = max(len(a), len(b))  # target length\n",
    "    first = a + [0.0] * (size - len(a))\n",
    "    second = b + [0.0] * (size - len(b))\n",
    "    return (first, second)  # output zero-padded copies\n",
    "\n",
    "class Document:  # minimal doc container\n",
    "    def __init__(self, doc_id: str, text: str) -> None:  # wire id/text fields\n",
    "        self.id = doc_id  # document id\n",
    "        self.text = text  # document text\n",
    "\n",
    "class LocalRetriever:  # deterministic in-memory retriever\n",
    "    def __init__(self, docs: list[Document]) -> None:  # store corpus and vocab\n",
    "        self.docs = docs  # corpus\n",
    "        self.vocab: dict[str, int] = {}  # shared vocabulary\n",
    "\n",
    "    def topk(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 2,\n",
    "    ) -> list[tuple[Document, float]]:\n",
    "        # compute cosine similarity and return hits\n",
    "        query_vec = bow(query, self.vocab)  # vectorize query\n",
    "        results: list[tuple[Document, float]] = []  # matching list\n",
    "        for doc in self.docs:  # scan each candidate document\n",
    "            doc_vec = bow(doc.text, self.vocab)  # vectorize doc\n",
    "            doc_vec, padded_query = pad(doc_vec, query_vec)  # ensure equal length\n",
    "            score = sum(x * y for x, y in zip(doc_vec, padded_query))\n",
    "            # cosine numerator\n",
    "            results.append((doc, float(score)))  # collect score\n",
    "        return sorted(results, key=lambda pair: pair[1], reverse=True)[:k]  # top-k\n",
    "\n",
    "documents = [  # tiny toy corpus\n",
    "    Document('s1', 'Alpha launched in 2022 with a focus on simplicity.'),\n",
    "    Document('s2', 'Key benefit: transparency in logs and short audits.'),\n",
    "    Document('s3', 'Beta emphasized speed over explainability in 2021.'),\n",
    "]  # sample corpus\n",
    "retriever = LocalRetriever(documents)  # create retriever\n",
    "print(retriever.topk('Alpha launch year', k=1))  # query demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52aaac5",
   "metadata": {
    "id": "plan"
   },
   "source": [
    "We define a tiny plan (constraints), draft bullets with citations, add a minimal fact-check pass, and compose the final brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dced682",
   "metadata": {
    "id": "draft"
   },
   "outputs": [],
   "source": [
    "def make_plan(topic: str) -> dict[str, object]:  # capture structure + constraints\n",
    "    \"\"\"Return plan metadata for the briefing.\"\"\"\n",
    "    return {\n",
    "        'topic': topic,\n",
    "        'bullets': 5,\n",
    "        'cite': '[source:id]',\n",
    "        'summary_len': '<=120',\n",
    "    }\n",
    "\n",
    "\n",
    "def draft_bullets(\n",
    "    topic: str,\n",
    "    hits: list[tuple[object, float]],\n",
    ") -> list[str]:\n",
    "    # convert hits into cite-ready bullets\n",
    "    \"\"\"Draft bullet list with simple inline citations.\"\"\"\n",
    "    bullets: list[str] = []  # accumulator\n",
    "    for doc, _score in hits:  # consume retrieval hits\n",
    "        clause = doc.text.split('.')[0]  # focus on first sentence\n",
    "        bullets.append(f\"- {clause} [source:{doc.id}]\")\n",
    "    if len(bullets) < 5 and hits:  # ensure at least five entries\n",
    "        fallback = f\"- Summary point on {topic} [source:{hits[0][0].id}]\"\n",
    "        # pad using top hit context\n",
    "        bullets.append(fallback)\n",
    "    return bullets[:5]\n",
    "\n",
    "\n",
    "def fact_check(\n",
    "    bullets: list[str],\n",
    "    docs_by_id: dict[str, object],\n",
    ") -> list[str]:\n",
    "    # tag bullets missing support\n",
    "    \"\"\"Append needs_review tag when no supporting doc line exists.\"\"\"\n",
    "    checked: list[str] = []  # results buffer\n",
    "    for bullet in bullets:  # scan each generated bullet\n",
    "        cites = re.findall(r'\\[source:([A-Za-z0-9_]+)\\]', bullet)\n",
    "        # extract cited ids\n",
    "        supported = any(  # ensure cited snippet appears in text\n",
    "            cite in docs_by_id and docs_by_id[cite].text.split('.')[0] in bullet\n",
    "            for cite in cites\n",
    "        )\n",
    "        entry = bullet if supported else f\"{bullet} [needs_review]\"\n",
    "        checked.append(entry)  # append flagged bullet\n",
    "    return checked\n",
    "\n",
    "\n",
    "def summary(bullets: list[str]) -> str:  # clip to <=120 chars\n",
    "    \"\"\"Return a short two-bullet summary clipped to 120 chars.\"\"\"\n",
    "    snippet = ' '.join(text.lstrip('- ') for text in bullets[:2])\n",
    "    # join first two bullets\n",
    "    return snippet if len(snippet) <= 120 else f\"{snippet[:117]}...\"\n",
    "\n",
    "\n",
    "topic = 'Alpha vs Beta'  # sample query\n",
    "hits = retriever.topk(topic, k=3)  # retrieve similar docs\n",
    "plan_spec = make_plan(topic)  # demonstrate structured plan metadata\n",
    "raw_bullets = draft_bullets(topic, hits)  # initial claims\n",
    "docs_by_id = {doc.id: doc for doc in documents}  # id -> document map\n",
    "checked_bullets = fact_check(raw_bullets, docs_by_id)  # label unsupported claims\n",
    "print(''.join(checked_bullets))  # display bullet list\n",
    "print('Summary:', summary(checked_bullets))  # show clipped summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7af4e",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}