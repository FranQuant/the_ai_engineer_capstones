{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb729fd-2d9f-4e9f-9f2a-465e9bccd2ed",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/Week02_backprop/01_numpy_manual.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee3e7b-5986-4df1-9379-5be0ae2c24a4",
   "metadata": {},
   "source": [
    "# Week 2 Capstone — Manual NumPy Backprop (1-Hidden-Layer MLP)\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "1. A tiny synthetic dataset  \n",
    "2. Manual forward pass (NumPy)  \n",
    "3. Manual backward pass via chain rule  \n",
    "4. Gradient check (finite differences)\n",
    "\n",
    "Following the *Deep Learning Basics: Chain Rule → Backprop → nn.Module* handout (TAE Program, Week 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002dee7c-05e0-48ca-bb1c-5ba3e6c163ff",
   "metadata": {},
   "source": [
    "## 1. Imports & Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f347168-4345-4049-bc02-08a760e1cf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy RNG initialized with seed = 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Deterministic seed\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    global rng\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"NumPy RNG initialized with seed =\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0abeb-fbcf-43c7-8e0e-c18accbb1364",
   "metadata": {},
   "source": [
    "## 2. Notebook Outline\n",
    "\n",
    "1. Synthetic dataset  \n",
    "2. Model definition (parameters, activation)  \n",
    "3. Forward pass  \n",
    "4. Loss function  \n",
    "5. Backward pass (manual gradients)  \n",
    "6. Gradient check (finite differences)  \n",
    "7. Sanity checks  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c30b1-92dd-4986-92af-caa30f42f4f4",
   "metadata": {},
   "source": [
    "## 3. (Tiny) Synthetic Dataset\n",
    "We implement the XOR-style pattern:\n",
    "\n",
    "$$\n",
    " x ~ U([-1, 1]^2), y = 1[x1 * x2 < 0]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1488337-dcf6-48cb-82e5-cd18e272ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sample:\n",
      " [[ 0.5479121  -0.12224312]\n",
      " [ 0.71719584  0.39473606]\n",
      " [-0.8116453   0.9512447 ]\n",
      " [ 0.5222794   0.57212861]]\n",
      "y sample:\n",
      " [1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Synthetic XOR-like dataset\n",
    "# ---------------------------------------\n",
    "# According to Section 6.1 of the Capstone:\n",
    "#   x ~ Uniform([-1, 1]^2)\n",
    "#   y = 1[x1 * x2 < 0]\n",
    "\n",
    "def generate_toy_data(n_samples=64):\n",
    "    X = rng.uniform(-1, 1, size=(n_samples, 2))\n",
    "    y = (X[:, 0] * X[:, 1] < 0).astype(float)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_toy_data(4)\n",
    "print(\"X sample:\\n\", X)\n",
    "print(\"y sample:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7288fa9-da62-495b-9988-2edac16679e5",
   "metadata": {},
   "source": [
    "## 4. Model Parameters (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c50376-63dd-4dbf-878a-2d470e9843ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: (4, 2)\n",
      "b1: (4,)\n",
      "W2: (1, 4)\n",
      "b2: (1,)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Model parameters: d -> h -> 1\n",
    "# ---------------------------------------\n",
    "\n",
    "d = 2     # input dimension\n",
    "h = 4     # hidden width\n",
    "out = 1   # scalar output\n",
    "\n",
    "# small Gaussian init to avoid saturation\n",
    "W1 = rng.normal(0, 0.1, size=(h, d))\n",
    "b1 = np.zeros((h,))\n",
    "\n",
    "W2 = rng.normal(0, 0.1, size=(1, h))\n",
    "b2 = np.zeros((1,))\n",
    "\n",
    "def get_params():\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "print(\"W1:\", W1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "print(\"W2:\", W2.shape)\n",
    "print(\"b2:\", b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cd433-79ca-4ae9-a3d8-2959c77fed40",
   "metadata": {},
   "source": [
    "## 5. Activation Functions (ReLU + ReLU')\n",
    "We use ReLU:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(u) = \\max(0, u)\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(u) = \n",
    "\\begin{cases}\n",
    "1 & u > 0 \\\\\n",
    "0 & u \\le 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e06f54-d182-4a43-8a41-de29d37fe6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(u):\n",
    "    return np.maximum(0, u)\n",
    "\n",
    "def relu_prime(u):\n",
    "    return (u > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89285921-aba3-4d8c-8772-73dbdeb15e29",
   "metadata": {},
   "source": [
    "## 6. Forward Pass (Single Sample)\n",
    "\n",
    "For a single sample $(x \\in \\mathbb{R}^d)$:\n",
    "\n",
    "$$\n",
    "a_1 = W_1 x + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = \\phi(a_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f = W_2 h + b_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6281d3b5-597b-4718-8185-5029f468aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_single(x, W1, b1, W2, b2):\n",
    "    a1 = W1 @ x + b1      # (h,)\n",
    "    h  = relu(a1)         # (h,)\n",
    "    f  = W2 @ h + b2      # (1,)\n",
    "    return a1, h, f[0]    # return scalar f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79a222-98e3-4f9d-a148-8586fd4ecdd4",
   "metadata": {},
   "source": [
    "## 7. Loss Function\n",
    "We use the squared error:\n",
    "\n",
    "$$\n",
    "L(f, y) = \\frac{1}{2}(f - y)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd484bda-ee8c-44c5-b6a5-80c06c6209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(f, y):\n",
    "    return 0.5 * (f - y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af482e-7c8f-403c-a09a-42b7c9b86d43",
   "metadata": {},
   "source": [
    "## 8. Manual Backward Pass (Chain Rule)\n",
    "\n",
    "Given\n",
    "\n",
    "- error: $ \\delta_f = f - y $\n",
    "- output gradient:\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W_2} = \\delta_f \\, h^{\\top},\n",
    "  \\qquad\n",
    "  \\frac{\\partial L}{\\partial b_2} = \\delta_f\n",
    "  $$\n",
    "- hidden layer:\n",
    "  $$\n",
    "  \\delta_h = W_2^\\top \\delta_f,\n",
    "  \\qquad\n",
    "  \\delta_{a_1} = \\delta_h \\odot \\phi'(a_1)\n",
    "  $$\n",
    "- input layer:\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W_1} = \\delta_{a_1} x^\\top,\n",
    "  \\qquad\n",
    "  \\frac{\\partial L}{\\partial b_1} = \\delta_{a_1}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd557b-20b1-4ae5-89e2-5c5b5d2c9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_single(x, y, a1, h, f, W1, b1, W2, b2):\n",
    "    # ----- output layer -----\n",
    "    df = f - y                # scalar\n",
    "\n",
    "    dW2 = df * h              # (h,)\n",
    "    db2 = df                  # scalar\n",
    "\n",
    "    # ----- hidden layer -----\n",
    "    # W2 comes as shape (1, h), so flatten it\n",
    "    dh = W2.flatten() * df           # (h,)\n",
    "    da1 = dh * relu_prime(a1)        # (h,)\n",
    "\n",
    "    # ----- input layer -----\n",
    "    dW1 = da1[:, None] @ x[None, :]  # (h, d)\n",
    "    db1 = da1\n",
    "\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7aeb19-dd5c-4748-a7d7-420be3f2d3c7",
   "metadata": {},
   "source": [
    "## 9. Single-Sample Sanity Test Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c030c3bf-daad-4021-a830-4cbca0a01c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output f = -0.0035382554995001636\n",
      "Loss L = 0.5035445151254901\n",
      "dW1 shape: (4, 2)\n",
      "db1 shape: (4,)\n",
      "dW2 shape: (4,)\n",
      "db2 scalar: -1.0035382554995003\n"
     ]
    }
   ],
   "source": [
    "# pick first sample\n",
    "x0, y0 = X[0], y[0]\n",
    "\n",
    "a1, h, f = forward_single(x0, W1, b1, W2, b2)\n",
    "L = mse_loss(f, y0)\n",
    "\n",
    "dW1, db1, dW2, db2 = backward_single(x0, y0, a1, h, f, W1, b1, W2, b2)\n",
    "\n",
    "print(\"Forward output f =\", f)\n",
    "print(\"Loss L =\", L)\n",
    "print(\"dW1 shape:\", dW1.shape)\n",
    "print(\"db1 shape:\", db1.shape)\n",
    "print(\"dW2 shape:\", dW2.shape)\n",
    "print(\"db2 scalar:\", db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddc004-ae1c-49ed-8f31-20fedd4328bc",
   "metadata": {},
   "source": [
    "## 10. Gradient Check (Finite Differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bcfd7-6e33-4b48-9f25-2e7bcf7380f5",
   "metadata": {},
   "source": [
    "### 10.1 Flatten / Unflatten Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6cbcf3-ac26-405b-b61e-842394362aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utilities to flatten and unflatten params\n",
    "# ---------------------------------------\n",
    "\n",
    "def flatten_params(W1, b1, W2, b2):\n",
    "    \"\"\"Flatten all parameters into a single 1D vector.\"\"\"\n",
    "    return np.concatenate([\n",
    "        W1.reshape(-1),\n",
    "        b1.reshape(-1),\n",
    "        W2.reshape(-1),\n",
    "        b2.reshape(-1),\n",
    "    ])\n",
    "\n",
    "def unflatten_params(theta, d=2, h=4):\n",
    "    \"\"\"Inverse of flatten_params.\"\"\"\n",
    "    # W1: (h, d)\n",
    "    size_W1 = h * d\n",
    "    W1 = theta[:size_W1].reshape(h, d)\n",
    "\n",
    "    # b1: (h,)\n",
    "    b1 = theta[size_W1:size_W1 + h]\n",
    "\n",
    "    # W2: (1, h)\n",
    "    start = size_W1 + h\n",
    "    size_W2 = h\n",
    "    W2 = theta[start:start + size_W2].reshape(1, h)\n",
    "\n",
    "    # b2: scalar\n",
    "    b2 = theta[start + size_W2]\n",
    "\n",
    "    return W1, b1, W2, np.array([b2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841154c-9e2b-4002-af79-3e34ab0845c0",
   "metadata": {},
   "source": [
    "### 10.2 Loss Wrapper for a Single Sample\n",
    "We define a function `loss_from_theta(theta, x, y)` so we can evaluate numeric derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0227c1d6-7a5c-4349-8632-67189f62dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Loss wrapper for flattened parameter vector\n",
    "# ---------------------------------------\n",
    "\n",
    "def loss_from_theta(theta, x, y):\n",
    "    W1, b1, W2, b2 = unflatten_params(theta)\n",
    "    a1, h, f = forward_single(x, W1, b1, W2, b2)\n",
    "    return mse_loss(f, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859ad6e-a7c2-449f-8ff2-4070749c8fcd",
   "metadata": {},
   "source": [
    "### 10.3 Analytic Gradient Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d351466c-8a74-4f00-a96a-e1e05fbd7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_grad(theta, x, y):\n",
    "    W1, b1, W2, b2 = unflatten_params(theta)\n",
    "    a1, h, f = forward_single(x, W1, b1, W2, b2)\n",
    "    dW1, db1, dW2, db2 = backward_single(x, y, a1, h, f, W1, b1, W2, b2)\n",
    "    return flatten_params(dW1, db1, dW2, np.array([db2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9be9b-1015-4cc0-acd6-299fecc0b6ee",
   "metadata": {},
   "source": [
    "### 10.4 Numeric Finite-Difference Gradient\n",
    "This implements:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_i} \\approx \\frac{L(\\theta_i+\\epsilon)-L(\\theta_i-\\epsilon)}{2\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f19f4344-ff13-47ff-a5a0-378ba071bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_grad(theta, x, y, eps=1e-5):\n",
    "    num_grads = np.zeros_like(theta)\n",
    "    for i in range(len(theta)):\n",
    "        theta_plus = theta.copy()\n",
    "        theta_minus = theta.copy()\n",
    "        theta_plus[i]  += eps\n",
    "        theta_minus[i] -= eps\n",
    "        \n",
    "        f_plus  = loss_from_theta(theta_plus, x, y)\n",
    "        f_minus = loss_from_theta(theta_minus, x, y)\n",
    "        \n",
    "        num_grads[i] = (f_plus - f_minus) / (2 * eps)\n",
    "    return num_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660c403-8120-4621-9185-a7e1676f9908",
   "metadata": {},
   "source": [
    "## 10.5 Gradient Check Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7958829e-cf02-4298-980c-4c75f19fe67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute diff:  9.519570028093671e-12\n",
      "Max relative diff:  8.721610611055463e-09\n",
      "\n",
      "Gradient check PASSED.\n"
     ]
    }
   ],
   "source": [
    "# pick one sample for checking\n",
    "x0, y0 = X[0], y[0]\n",
    "\n",
    "theta = flatten_params(W1, b1, W2, b2)\n",
    "\n",
    "g_analytic = analytic_grad(theta, x0, y0)\n",
    "g_numeric  = numeric_grad(theta, x0, y0)\n",
    "\n",
    "abs_diff = np.abs(g_analytic - g_numeric)\n",
    "rel_diff = abs_diff / (np.abs(g_numeric) + 1e-8)\n",
    "\n",
    "print(\"Max absolute diff: \", abs_diff.max())\n",
    "print(\"Max relative diff: \", rel_diff.max())\n",
    "\n",
    "# Fail loudly if wrong\n",
    "assert abs_diff.max() < 1e-5, \"Gradient check FAILED!\"\n",
    "print(\"\\nGradient check PASSED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1d52e-368d-406e-830a-c03d954e231e",
   "metadata": {},
   "source": [
    "## 11. Conclusion — What We Achieved\n",
    "\n",
    "In this notebook we built a complete 2-layer neural network **from scratch in NumPy**, with **no autograd** and **no frameworks**.  \n",
    "The core objective was to expose and verify the mathematical mechanics underlying backpropagation.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "- We implemented all model components manually:\n",
    "  - Parameters \\((W_1, b_1, W_2, b_2)\\)\n",
    "  - ReLU activation\n",
    "  - Forward computation  \n",
    "  - Mean-squared error loss\n",
    "\n",
    "- We derived **analytical gradients** for every parameter by hand, using:\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W_2}, \\quad \n",
    "  \\frac{\\partial L}{\\partial b_2}, \\quad\n",
    "  \\frac{\\partial L}{\\partial W_1}, \\quad\n",
    "  \\frac{\\partial L}{\\partial b_1}.\n",
    "  $$\n",
    "\n",
    "- We implemented a **finite-difference gradient checker**, using:\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\theta_i}\n",
    "  \\approx \n",
    "  \\frac{L(\\theta_i + \\varepsilon) - L(\\theta_i - \\varepsilon)}\n",
    "       {2\\varepsilon}.\n",
    "  $$\n",
    "\n",
    "- The analytical gradients matched the numerical ones to machine precision:\n",
    "  - Max absolute diff ≈ \\(10^{-11}\\)  \n",
    "  - Max relative diff ≈ \\(10^{-9}\\)\n",
    "\n",
    "This verifies that our backward-pass derivations and implementation are **correct**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
