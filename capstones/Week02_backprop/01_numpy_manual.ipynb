{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb729fd-2d9f-4e9f-9f2a-465e9bccd2ed",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/Week02_backprop/01_numpy_manual.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee3e7b-5986-4df1-9379-5be0ae2c24a4",
   "metadata": {},
   "source": [
    "# 01 — Manual NumPy Backprop (1-Hidden-Layer MLP)\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "1. A tiny synthetic dataset  \n",
    "2. Manual forward pass (NumPy)  \n",
    "3. Manual backward pass via chain rule  \n",
    "4. Gradient check (finite differences)\n",
    "\n",
    "Following the *Deep Learning Basics: Chain Rule → Backprop → nn.Module* handout (TAE Program, Week 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002dee7c-05e0-48ca-bb1c-5ba3e6c163ff",
   "metadata": {},
   "source": [
    "## 1. Imports & Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f347168-4345-4049-bc02-08a760e1cf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy RNG initialized with seed = 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Deterministic seed\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED) \n",
    "                                \n",
    "def set_seed(seed=42):\n",
    "    global rng\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"NumPy RNG initialized with seed =\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c30b1-92dd-4986-92af-caa30f42f4f4",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset\n",
    "We implement the XOR-style pattern:\n",
    "\n",
    "$$\n",
    " x ~ U([-1, 1]^2), y = 1[x1 * x2 < 0]\n",
    "$$\n",
    "\n",
    "The dataset construction logic follows the standard XOR quadrant logic.\n",
    "\n",
    "- Each sample is drawn uniformly from the square  \n",
    "  $$[-1, 1]^2.$$\n",
    "\n",
    "- The binary label is generated using the rule  \n",
    "  $$\n",
    "  y = \\mathbf{1}[\\,x_1 x_2 < 0\\,],\n",
    "  $$\n",
    "  which captures the XOR pattern  \n",
    "  (points where the two coordinates have opposite signs).\n",
    "\n",
    "\n",
    "- Shapes:\n",
    "  - Feature matrix  \n",
    "    $$X \\in \\mathbb{R}^{n_{\\text{samples}} \\times 2}$$\n",
    "  - Label vector  \n",
    "    $$y \\in \\mathbb{R}^{n_{\\text{samples}}}$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1488337-dcf6-48cb-82e5-cd18e272ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sample:\n",
      " [[ 0.5479121  -0.12224312]\n",
      " [ 0.71719584  0.39473606]\n",
      " [-0.8116453   0.9512447 ]\n",
      " [ 0.5222794   0.57212861]]\n",
      "y sample:\n",
      " [1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Synthetic XOR-like dataset\n",
    "# ---------------------------------------\n",
    "# According to Section 6.1 of the Capstone:\n",
    "#   x ~ Uniform([-1, 1]^2)\n",
    "#   y = 1[x1 * x2 < 0]\n",
    "\n",
    "def generate_toy_data(n_samples=64):\n",
    "    X = rng.uniform(-1, 1, size=(n_samples, 2))\n",
    "    y = (X[:, 0] * X[:, 1] < 0).astype(float)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_toy_data(4)\n",
    "print(\"X sample:\\n\", X)\n",
    "print(\"y sample:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7288fa9-da62-495b-9988-2edac16679e5",
   "metadata": {},
   "source": [
    "## 3. Model Parameters (NumPy)\n",
    "Parameter initializations:\n",
    "\n",
    "- $W_1$: shape $(h, d)$  \n",
    "- $b_1$: shape $(h,)$  \n",
    "- $W_2$: shape $(1, h)$  \n",
    "- $b_2$: shape $(1,)$  \n",
    "\n",
    "These are mathematically consistent with a 1-hidden-layer MLP:\n",
    "\n",
    "- Input vector  \n",
    "  $$x \\in \\mathbb{R}^2$$\n",
    "\n",
    "- First affine layer  \n",
    "  $$\n",
    "  W_1 x + b_1 \\in \\mathbb{R}^h\n",
    "  $$\n",
    "\n",
    "- Activation  \n",
    "  $$\n",
    "  h = \\text{ReLU}(W_1 x + b_1)\n",
    "  $$\n",
    "\n",
    "- Output layer  \n",
    "  $$\n",
    "  W_2 h + b_2 \\in \\mathbb{R}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c50376-63dd-4dbf-878a-2d470e9843ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: (4, 2)\n",
      "b1: (4,)\n",
      "W2: (1, 4)\n",
      "b2: (1,)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Model parameters: d -> h -> 1\n",
    "# ---------------------------------------\n",
    "\n",
    "d = 2     # input dimension\n",
    "h = 4     # hidden width\n",
    "out = 1   # scalar output\n",
    "\n",
    "# small Gaussian init to avoid saturation\n",
    "W1 = rng.normal(0, 0.1, size=(h, d))\n",
    "b1 = np.zeros((h,))\n",
    "\n",
    "W2 = rng.normal(0, 0.1, size=(1, h))\n",
    "b2 = np.zeros((1,))\n",
    "\n",
    "def get_params():\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "print(\"W1:\", W1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "print(\"W2:\", W2.shape)\n",
    "print(\"b2:\", b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cd433-79ca-4ae9-a3d8-2959c77fed40",
   "metadata": {},
   "source": [
    "## 4. Activation Functions (ReLU + ReLU')\n",
    "We use the Rectified Linear Unit:\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(u) = \\max(0, u),\n",
    "$$\n",
    "\n",
    "applied elementwise.  \n",
    "Its derivative is\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}'(u) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } u > 0, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note: ReLU is **non-differentiable at $u = 0$**; this will introduce subtle\n",
    "issues during numerical gradient checking near zero-activation units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e06f54-d182-4a43-8a41-de29d37fe6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Activation: ReLU and ReLU'\n",
    "# ---------------------------------------\n",
    "\n",
    "def relu(u):\n",
    "    \"\"\"\n",
    "    Elementwise ReLU.\n",
    "    Input:\n",
    "        u : array-like (any shape)\n",
    "    Returns:\n",
    "        out : same shape as u\n",
    "    \"\"\"\n",
    "    return np.maximum(0, u)\n",
    "\n",
    "\n",
    "def relu_prime(u):\n",
    "    \"\"\"\n",
    "    Elementwise derivative of ReLU.\n",
    "    Returns 1 where u > 0, else 0.\n",
    "\n",
    "    Note:\n",
    "    The derivative is undefined at u == 0; we follow the standard\n",
    "    subgradient convention used in deep learning frameworks.\n",
    "    \"\"\"\n",
    "    return (u > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89285921-aba3-4d8c-8772-73dbdeb15e29",
   "metadata": {},
   "source": [
    "## 5. Forward Pass (Single Sample)\n",
    "For a single input $ x \\in \\mathbb{R}^d $, the 1-hidden-layer MLP computes:\n",
    "\n",
    "1. First affine layer  \n",
    "$$\n",
    "a_1 = W_1 x + b_1 \\quad\\in \\mathbb{R}^h\n",
    "$$\n",
    "\n",
    "2. Activation  \n",
    "$$\n",
    "h = \\phi(a_1) = \\mathrm{ReLU}(a_1)\n",
    "$$\n",
    "\n",
    "3. Output layer  \n",
    "$$\n",
    "f = W_2 h + b_2 \\quad\\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "We return the intermediate values to facilitate backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6281d3b5-597b-4718-8185-5029f468aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Forward pass: single sample\n",
    "# ---------------------------------------\n",
    "\n",
    "def forward_single(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass for one input sample.\n",
    "\n",
    "    Inputs:\n",
    "        x  : (d,)      input vector\n",
    "        W1 : (h, d)\n",
    "        b1 : (h,)\n",
    "        W2 : (1, h)\n",
    "        b2 : (1,)\n",
    "\n",
    "    Returns:\n",
    "        a1 : (h,)      pre-activation\n",
    "        h  : (h,)      hidden layer activation (ReLU)\n",
    "        f  : float     scalar model output\n",
    "    \"\"\"\n",
    "    a1 = W1 @ x + b1        # shape (h,)\n",
    "    h  = relu(a1)           # shape (h,)\n",
    "    f  = W2 @ h + b2        # shape (1,), array\n",
    "    return a1, h, f.item()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79a222-98e3-4f9d-a148-8586fd4ecdd4",
   "metadata": {},
   "source": [
    "## 6. Loss Function\n",
    "We use the scalar squared-error loss\n",
    "\n",
    "$$\n",
    "L(f, y) = \\frac{1}{2}(f - y)^2.\n",
    "$$\n",
    "\n",
    "It is differentiable with respect to the scalar model output $f$, and  \n",
    "its derivative will appear in the backward pass:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial f} = f - y.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd484bda-ee8c-44c5-b6a5-80c06c6209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Loss function: scalar squared error\n",
    "# ---------------------------------------\n",
    "\n",
    "def loss_fn(f_scalar, y_scalar):\n",
    "    \"\"\"\n",
    "    Compute L = 0.5 * (f - y)^2 for scalar f, y.\n",
    "    \n",
    "    Inputs:\n",
    "        f_scalar : float\n",
    "        y_scalar : float\n",
    "    Returns:\n",
    "        L        : float\n",
    "    \"\"\"\n",
    "    return 0.5 * (f_scalar - y_scalar) ** 2\n",
    "\n",
    "def loss_grad_f(f_scalar, y_scalar):\n",
    "    \"\"\"\n",
    "    Derivative dL/df = (f - y).\n",
    "    \"\"\"\n",
    "    return f_scalar - y_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af482e-7c8f-403c-a09a-42b7c9b86d43",
   "metadata": {},
   "source": [
    "## 7. Manual Backward Pass (Chain Rule)\n",
    "\n",
    "We apply the chain rule to compute the gradients of the loss\n",
    "$$\n",
    "L = \\tfrac12 (f - y)^2\n",
    "$$\n",
    "with respect to all parameters.\n",
    "\n",
    "**Output layer**\n",
    "$$\n",
    "\\delta_f = f - y,\n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial W_2} = \\delta_f \\, h^\\top,\n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b_2} = \\delta_f.\n",
    "$$\n",
    "\n",
    "**Hidden layer**\n",
    "$$\n",
    "\\delta_h = W_2^\\top \\, \\delta_f,\n",
    "\\qquad\n",
    "\\delta_{a_1} = \\delta_h \\odot \\phi'(a_1).\n",
    "$$\n",
    "\n",
    "**Input layer**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\delta_{a_1} \\, x^\\top,\n",
    "\\qquad\n",
    "\\frac{\\partial L}{\\partial b_1} = \\delta_{a_1}.\n",
    "$$\n",
    "\n",
    "All gradient arrays match the shapes of their corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd557b-20b1-4ae5-89e2-5c5b5d2c9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Backward pass: single sample (manual)\n",
    "# ---------------------------------------\n",
    "\n",
    "def backward_single(x, y, a1, h, f, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Manual backward pass using the chain rule.\n",
    "\n",
    "    Inputs:\n",
    "        x  : (d,)\n",
    "        y  : float\n",
    "        a1 : (h,)\n",
    "        h  : (h,)\n",
    "        f  : float\n",
    "        W1 : (h, d)\n",
    "        b1 : (h,)\n",
    "        W2 : (1, h)\n",
    "        b2 : (1,)\n",
    "\n",
    "    Returns:\n",
    "        dW1 : (h, d)\n",
    "        db1 : (h,)\n",
    "        dW2 : (1, h)\n",
    "        db2 : (1,)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- output layer -----\n",
    "    df = f - y                      # scalar (dL/df)\n",
    "\n",
    "    dW2 = df * h[None, :]           # (1, h) correctly matches W2\n",
    "    db2 = np.array([df])            # (1,) matches b2\n",
    "\n",
    "    # ----- hidden layer -----\n",
    "    dh  = W2[0] * df                # W2[0] is (h,) -> produces (h,)\n",
    "    da1 = dh * relu_prime(a1)       # (h,)\n",
    "\n",
    "    # ----- input layer -----\n",
    "    dW1 = da1[:, None] @ x[None, :] # (h, d)\n",
    "    db1 = da1                       # (h,)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7aeb19-dd5c-4748-a7d7-420be3f2d3c7",
   "metadata": {},
   "source": [
    "## 8. Single-Sample Sanity Check\n",
    "\n",
    "Before performing numerical gradient checking, we verify that the\n",
    "manual forward and backward passes run end-to-end without shape or\n",
    "broadcasting errors.\n",
    "\n",
    "For a single data point $(x_0, y_0)$, we compute:\n",
    "\n",
    "- forward activations and loss,\n",
    "- all parameter gradients from the backward pass,\n",
    "- and confirm that each gradient has the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c030c3bf-daad-4021-a830-4cbca0a01c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output f = -0.0035382554995001636\n",
      "Loss L = 0.5035445151254901\n",
      "dW1 shape: (4, 2)\n",
      "db1 shape: (4,)\n",
      "dW2 shape: (1, 4)\n",
      "db2 shape: (1,) value: [-1.00353826]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Sanity check: forward + backward (single sample)\n",
    "# ---------------------------------------\n",
    "\n",
    "# pick first sample\n",
    "x0, y0 = X[0], y[0]\n",
    "\n",
    "# forward pass\n",
    "a1, h, f = forward_single(x0, W1, b1, W2, b2)\n",
    "L = loss_fn(f, y0)\n",
    "\n",
    "# backward pass\n",
    "dW1, db1, dW2, db2 = backward_single(x0, y0, a1, h, f, W1, b1, W2, b2)\n",
    "\n",
    "# print diagnostics\n",
    "print(\"Forward output f =\", f)\n",
    "print(\"Loss L =\", L)\n",
    "print(\"dW1 shape:\", dW1.shape)\n",
    "print(\"db1 shape:\", db1.shape)\n",
    "print(\"dW2 shape:\", dW2.shape)\n",
    "print(\"db2 shape:\", db2.shape, \"value:\", db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bcfd7-6e33-4b48-9f25-2e7bcf7380f5",
   "metadata": {},
   "source": [
    "## 9. Gradient Check (Finite Differences)\n",
    "\n",
    "### 9.1 Parameter Flattening and Unflattening\n",
    "\n",
    "To perform numerical gradient checking, we need to flatten all model\n",
    "parameters into a single vector `theta`, perturb coordinates one by one,\n",
    "and reconstruct the parameter tensors from the perturbed vector.\n",
    "\n",
    "A robust implementation must:\n",
    "- preserve the **exact original shapes** of each tensor,\n",
    "- avoid hardcoded dimensions (`d`, `h`),\n",
    "- avoid silent shape mismatches,\n",
    "- ensure `b2` always has shape `(1,)`.\n",
    "\n",
    "We therefore implement dimension-agnostic flatten/unflatten utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6cbcf3-ac26-405b-b61e-842394362aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utilities: flatten/unflatten parameters\n",
    "# ---------------------------------------\n",
    "\n",
    "def flatten_params(W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Flatten all parameters into a single 1D array.\n",
    "    Parameters must already have shapes:\n",
    "        W1 : (h, d)\n",
    "        b1 : (h,)\n",
    "        W2 : (1, h)\n",
    "        b2 : (1,)\n",
    "    \"\"\"\n",
    "    return np.concatenate([\n",
    "        W1.reshape(-1),\n",
    "        b1.reshape(-1),\n",
    "        W2.reshape(-1),\n",
    "        b2.reshape(-1),\n",
    "    ])\n",
    "\n",
    "\n",
    "def unflatten_params(theta, W1_shape, b1_shape, W2_shape, b2_shape):\n",
    "    \"\"\"\n",
    "    Reconstruct parameters from a flattened theta vector.\n",
    "\n",
    "    Shapes must be passed explicitly to avoid hardcoded dimensions.\n",
    "    \"\"\"\n",
    "    # Compute sizes\n",
    "    size_W1 = np.prod(W1_shape)\n",
    "    size_b1 = np.prod(b1_shape)\n",
    "    size_W2 = np.prod(W2_shape)\n",
    "    size_b2 = np.prod(b2_shape)\n",
    "\n",
    "    # Slicing indices\n",
    "    i0 = 0\n",
    "    i1 = i0 + size_W1\n",
    "    i2 = i1 + size_b1\n",
    "    i3 = i2 + size_W2\n",
    "    i4 = i3 + size_b2  # final index (redundant, but clean)\n",
    "\n",
    "    # Unflatten\n",
    "    W1 = theta[i0:i1].reshape(W1_shape)\n",
    "    b1 = theta[i1:i2].reshape(b1_shape)\n",
    "    W2 = theta[i2:i3].reshape(W2_shape)\n",
    "    b2 = theta[i3:i4].reshape(b2_shape)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841154c-9e2b-4002-af79-3e34ab0845c0",
   "metadata": {},
   "source": [
    "### 9.2 Loss Wrapper for a Single Sample\n",
    "\n",
    "For numerical gradient checking, we require a function\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = L(f(\\theta; x), y)\n",
    "$$\n",
    "\n",
    "that takes a flattened parameter vector `theta`, reconstructs the model\n",
    "parameters, runs a forward pass on a single sample `(x, y)`, and returns\n",
    "the scalar loss. This isolates the dependence on `theta` and enables\n",
    "finite-difference perturbations for each parameter coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0227c1d6-7a5c-4349-8632-67189f62dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Loss wrapper for a flattened parameter vector\n",
    "# ---------------------------------------\n",
    "\n",
    "def loss_from_theta(theta, x, y,\n",
    "                    W1_shape, b1_shape,\n",
    "                    W2_shape, b2_shape):\n",
    "    \"\"\"\n",
    "    Compute L(theta) for a single sample (x, y):\n",
    "\n",
    "        1. Unflatten theta back into W1, b1, W2, b2\n",
    "        2. Forward pass to compute f\n",
    "        3. Return scalar loss L = 0.5 * (f - y)^2\n",
    "\n",
    "    Shapes are passed explicitly to avoid hardcoded dimensions.\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = unflatten_params(\n",
    "        theta,\n",
    "        W1_shape, b1_shape,\n",
    "        W2_shape, b2_shape\n",
    "    )\n",
    "    \n",
    "    a1, h, f = forward_single(x, W1, b1, W2, b2)\n",
    "    return loss_fn(f, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859ad6e-a7c2-449f-8ff2-4070749c8fcd",
   "metadata": {},
   "source": [
    "### 9.3 Analytic Gradient (Flattened)\n",
    "\n",
    "To compare analytic and numerical gradients, we need the analytic gradient\n",
    "expressed as a single flattened vector. This requires:\n",
    "\n",
    "1. Unflattening `theta` into `(W1, b1, W2, b2)`\n",
    "2. Computing all parameter gradients via `backward_single`\n",
    "3. Flattening the resulting gradients back into a 1D vector\n",
    "\n",
    "All tensor shapes are passed explicitly to avoid hardcoded assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d351466c-8a74-4f00-a96a-e1e05fbd7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Analytic gradient, flattened\n",
    "# ---------------------------------------\n",
    "\n",
    "def analytic_grad(theta, x, y,\n",
    "                  W1_shape, b1_shape,\n",
    "                  W2_shape, b2_shape):\n",
    "    \"\"\"\n",
    "    Compute the analytic gradient dL/dtheta as a flattened 1D vector.\n",
    "\n",
    "    Steps:\n",
    "    1. Unflatten theta -> W1, b1, W2, b2\n",
    "    2. Forward pass to compute activations\n",
    "    3. Backward pass to compute gradients\n",
    "    4. Flatten gradients back into a single vector\n",
    "    \"\"\"\n",
    "    # unflatten with explicit shapes\n",
    "    W1, b1, W2, b2 = unflatten_params(\n",
    "        theta,\n",
    "        W1_shape, b1_shape,\n",
    "        W2_shape, b2_shape\n",
    "    )\n",
    "\n",
    "    # forward + backward\n",
    "    a1, h, f = forward_single(x, W1, b1, W2, b2)\n",
    "    dW1, db1, dW2, db2 = backward_single(\n",
    "        x, y, a1, h, f, W1, b1, W2, b2\n",
    "    )\n",
    "\n",
    "    # flatten gradients (db2 already shape (1,))\n",
    "    return flatten_params(dW1, db1, dW2, db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9be9b-1015-4cc0-acd6-299fecc0b6ee",
   "metadata": {},
   "source": [
    "### 9.4 Numerical Gradient (Central Differences)\n",
    "\n",
    "We approximate each partial derivative of the loss with respect to the\n",
    "flattened parameter vector `theta` using the centered finite-difference:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_i} \\approx\n",
    "\\frac{L(\\theta + \\varepsilon e_i) -\n",
    "      L(\\theta - \\varepsilon e_i)}{2\\varepsilon}.\n",
    "$$\n",
    "\n",
    "Due to ReLU’s non-differentiability at zero, coordinates that place any\n",
    "hidden pre-activation \\(a_1\\) exactly at zero can yield noisy numerical\n",
    "gradients; we skip such coordinates during checking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f19f4344-ff13-47ff-a5a0-378ba071bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Numerical gradient (central differences)\n",
    "# ---------------------------------------\n",
    "\n",
    "def numeric_grad(theta, x, y,\n",
    "                 W1_shape, b1_shape,\n",
    "                 W2_shape, b2_shape,\n",
    "                 eps=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient of L(theta) via central differences.\n",
    "\n",
    "    Inputs:\n",
    "        theta : (P,) flattened parameter vector\n",
    "        x, y  : single sample\n",
    "        eps   : finite-difference step size\n",
    "        shape arguments : used for unflattening\n",
    "\n",
    "    Returns:\n",
    "        num_grads : (P,) numerical gradient\n",
    "    \"\"\"\n",
    "    num_grads = np.zeros_like(theta)\n",
    "    theta_plus  = theta.copy()\n",
    "    theta_minus = theta.copy()\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        # finite-difference perturbations\n",
    "        theta_plus[i]  = theta[i] + eps\n",
    "        theta_minus[i] = theta[i] - eps\n",
    "\n",
    "        # compute losses\n",
    "        f_plus = loss_from_theta(\n",
    "            theta_plus, x, y,\n",
    "            W1_shape, b1_shape,\n",
    "            W2_shape, b2_shape\n",
    "        )\n",
    "        f_minus = loss_from_theta(\n",
    "            theta_minus, x, y,\n",
    "            W1_shape, b1_shape,\n",
    "            W2_shape, b2_shape\n",
    "        )\n",
    "\n",
    "        num_grads[i] = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "        # restore coordinates\n",
    "        theta_plus[i]  = theta[i]\n",
    "        theta_minus[i] = theta[i]\n",
    "\n",
    "    return num_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660c403-8120-4621-9185-a7e1676f9908",
   "metadata": {},
   "source": [
    "### 9.5 Gradient Check Comparison\n",
    "\n",
    "We now compare the analytic gradient vector with the numerical finite-difference\n",
    "gradient. For well-behaved (non-ReLU-boundary) coordinates, the maximum absolute\n",
    "difference should be on the order of machine precision (≈1e−6 to 1e−7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7958829e-cf02-4298-980c-4c75f19fe67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute diff:  9.519570028093671e-12\n",
      "Max relative diff:  8.721610611055463e-09\n",
      "\n",
      "Top-5 largest coordinate diffs:\n",
      "  idx 9: analytic=0.096228, numeric=0.096228, abs diff=7.995535e-12\n",
      "  idx 13: analytic=-0.038812, numeric=-0.038812, abs diff=8.153749e-12\n",
      "  idx 0: analytic=-0.020276, numeric=-0.020276, abs diff=8.851111e-12\n",
      "  idx 12: analytic=-0.009541, numeric=-0.009541, abs diff=9.498794e-12\n",
      "  idx 11: analytic=0.005010, numeric=0.005010, abs diff=9.519570e-12\n",
      "\n",
      "Gradient check PASSED.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Gradient check: analytic vs numeric\n",
    "# ---------------------------------------\n",
    "\n",
    "# pick sample\n",
    "x0, y0 = X[0], y[0]\n",
    "\n",
    "# store parameter shapes\n",
    "W1_shape, b1_shape = W1.shape, b1.shape\n",
    "W2_shape, b2_shape = W2.shape, b2.shape\n",
    "\n",
    "# flatten initial parameters\n",
    "theta = flatten_params(W1, b1, W2, b2)\n",
    "\n",
    "# compute analytic and numeric gradients\n",
    "g_analytic = analytic_grad(\n",
    "    theta, x0, y0,\n",
    "    W1_shape, b1_shape,\n",
    "    W2_shape, b2_shape\n",
    ")\n",
    "\n",
    "g_numeric = numeric_grad(\n",
    "    theta, x0, y0,\n",
    "    W1_shape, b1_shape,\n",
    "    W2_shape, b2_shape\n",
    ")\n",
    "\n",
    "# diagnostics\n",
    "abs_diff = np.abs(g_analytic - g_numeric)\n",
    "rel_diff = abs_diff / (np.abs(g_numeric) + 1e-8)\n",
    "\n",
    "print(\"Max absolute diff: \", abs_diff.max())\n",
    "print(\"Max relative diff: \", rel_diff.max())\n",
    "\n",
    "# Helpful diagnostic: show top offending coordinates\n",
    "top_k = np.argsort(abs_diff)[-5:]\n",
    "print(\"\\nTop-5 largest coordinate diffs:\")\n",
    "for idx in top_k:\n",
    "    print(f\"  idx {idx}: analytic={g_analytic[idx]:.6f}, numeric={g_numeric[idx]:.6f}, abs diff={abs_diff[idx]:.6e}\")\n",
    "\n",
    "# fail loudly if wrong (skip extremely small ReLU-boundary anomalies)\n",
    "assert abs_diff.max() < 1e-5, \"Gradient check FAILED!\"\n",
    "print(\"\\nGradient check PASSED.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
