{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397447dd-88ba-4d25-81fb-bfaba757c7cf",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/FranQuant/the_ai_engineer_capstones/blob/main/capstones/Week02_backprop/02_pytorch_no_autograd.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce788974-bae7-48f4-9817-1ee550ef8459",
   "metadata": {},
   "source": [
    "# 02 — PyTorch Implementation (No Autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ee607-da99-415c-b6ca-5b4d4f37f73a",
   "metadata": {},
   "source": [
    "## 1. Imports & Deterministic Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a134e7-b3d7-4fff-8332-2a0d14bdfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Imports & Deterministic Seeds (match NB01)\n",
    "# ============================================\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392240b-ccf5-43c5-8555-8a2dd50f1316",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset (same as Notebook 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21dc0422-762c-4391-942b-9fe757c6ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Synthetic Dataset (match NB01 exactly)\n",
    "# ============================================\n",
    "N = 500  # same as Notebook 01\n",
    "\n",
    "X_np = rng.uniform(-1, 1, size=(N, 2))\n",
    "y_np = (X_np[:, 0] * X_np[:, 1] < 0).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch (float32 everywhere)\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3a172-21d1-4096-877e-14eca1a27144",
   "metadata": {},
   "source": [
    "## 3. NumPy Reference Forward Pass (from Notebook 01)\n",
    "To ensure numerical parity between the NumPy and PyTorch implementations,\n",
    "we replicate the minimal forward-pass functions from Notebook 01. These\n",
    "are used for direct comparison in Section 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e253fc2b-2e73-474f-8cf2-aa9fba8062cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. NumPy Reference Forward Pass (from NB01)\n",
    "# ============================================\n",
    "\n",
    "def relu_np(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_single(x, W1, b1, W2, b2):\n",
    "    a1 = W1 @ x + b1        # (h,)\n",
    "    h  = relu_np(a1)        # (h,)\n",
    "    f  = W2 @ h + b2        # (1,)\n",
    "    return a1, h, float(f[0])  # explicit scalar extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f9bb4-49a9-406a-897d-bde7c9f292f3",
   "metadata": {},
   "source": [
    "## 4 .NumPy Model Parameters (for comparison)\n",
    "\n",
    "Notebook 02 needs standalone NumPy parameters to reproduce the exact\n",
    "forward pass used in Notebook 01. These are synchronized with the\n",
    "PyTorch parameters in Section 5 so both implementations produce\n",
    "identical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e2fb68-9e2e-4a91-8186-f30284d2d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. NumPy Model Parameters (match NB01 exactly)\n",
    "# ============================================\n",
    "\n",
    "d, h = 2, 4\n",
    "\n",
    "# Weight initialization: small Gaussian (std = 0.1), cast to float32\n",
    "W1 = rng.normal(0.0, 0.1, size=(h, d)).astype(np.float32)\n",
    "W2 = rng.normal(0.0, 0.1, size=(1, h)).astype(np.float32)\n",
    "\n",
    "# Biases as float32\n",
    "b1 = np.zeros((h,), dtype=np.float32)\n",
    "b2 = np.zeros((1,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312d2b3-776b-4436-8e37-5ec76979071f",
   "metadata": {},
   "source": [
    "## 5. Model Parameters in PyTorch (No Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9febf33a-c316-4b3e-88cd-0deaacfaebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. PyTorch Parameters (synced with NumPy)\n",
    "# ============================================\n",
    "\n",
    "# Create tensors with the same shapes and dtypes as NumPy params\n",
    "W1_t = torch.empty((h, d), dtype=torch.float32)\n",
    "b1_t = torch.empty((h,),   dtype=torch.float32)\n",
    "W2_t = torch.empty((1, h), dtype=torch.float32)\n",
    "b2_t = torch.empty((1,),   dtype=torch.float32)\n",
    "\n",
    "# Disable autograd for this notebook\n",
    "for t in [W1_t, b1_t, W2_t, b2_t]:\n",
    "    t.requires_grad_(False)\n",
    "\n",
    "# Sync PyTorch parameters with NumPy (safe in-place copy)\n",
    "W1_t.copy_(torch.tensor(W1, dtype=torch.float32))\n",
    "b1_t.copy_(torch.tensor(b1, dtype=torch.float32))\n",
    "W2_t.copy_(torch.tensor(W2, dtype=torch.float32))\n",
    "b2_t.copy_(torch.tensor(b2, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1b9d-f685-4c26-a749-3ad5cbdc7e7d",
   "metadata": {},
   "source": [
    "## 6. Activation Function\n",
    "#### Match NumPy ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6711a7ed-e190-4553-95cf-dd87b74abb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_t(x):\n",
    "    return torch.clamp(x, min=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed0b5e-44c8-454b-bb1a-ada378e8c3f9",
   "metadata": {},
   "source": [
    "## 7. Forward Pass (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac5e5a0-01eb-4cd7-9981-0fec9f791a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Forward Pass (PyTorch) — single-sample only\n",
    "# ============================================\n",
    "\n",
    "def forward_torch(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Single-sample forward pass matching NumPy forward_single.\n",
    "    x:  (d,)\n",
    "    W1: (h, d)\n",
    "    b1: (h,)\n",
    "    W2: (1, h)\n",
    "    b2: (1,)\n",
    "    Returns:\n",
    "        a1: pre-activation (h,)\n",
    "        h:  hidden layer (h,)\n",
    "        f:  scalar output (float)\n",
    "    \"\"\"\n",
    "    a1 = x @ W1.T + b1        # (h,)\n",
    "    h  = relu_t(a1)\n",
    "    f  = W2 @ h + b2\n",
    "    return a1, h, f.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b4c5b-1e6a-4b9f-ba11-acf6375a4199",
   "metadata": {},
   "source": [
    "## 8. Loss Function\n",
    "#### Match NumPy MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aafde80-f281-4d57-aed2-8de231a950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. Loss Function (match NumPy exactly)\n",
    "# ============================================\n",
    "\n",
    "def mse_loss_t(f, y):\n",
    "    \"\"\"\n",
    "    Mean-squared error loss for a single sample.\n",
    "    Matches the NumPy definition L = 0.5 * (f - y)**2.\n",
    "    f: Python float\n",
    "    y: torch scalar tensor (float32)\n",
    "    Returns: torch scalar tensor\n",
    "    \"\"\"\n",
    "    return 0.5 * (f - y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d6636-ed17-41ed-908f-2065b81396ee",
   "metadata": {},
   "source": [
    "## 9. Numerical Consistency Test (NumPy vs Torch)\n",
    "We compare the NumPy output from Notebook 01 with the PyTorch output here.\n",
    "\n",
    "Pick a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97e4eca-5d37-4092-81e4-46a140c6fc52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy output f_np = -0.0017306688285198745\n",
      "Torch output f_t = -0.0017306690569967031\n",
      "Absolute difference = 2.2847682860233087e-10\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 9. Numerical Consistency Test (NumPy vs Torch)\n",
    "# ============================================\n",
    "\n",
    "i = 0  # pick sample\n",
    "x_i_np = X_np[i]              # NumPy input\n",
    "y_i   = y[i]                  # Torch scalar tensor target\n",
    "\n",
    "# NumPy forward pass\n",
    "a1_np, h_np, f_np = forward_single(x_i_np, W1, b1, W2, b2)   # scalar f_np (Python float)\n",
    "\n",
    "# PyTorch forward pass\n",
    "x_i_t = X[i]                  # torch.float32\n",
    "a1_t, h_t, f_t = forward_torch(x_i_t, W1_t, b1_t, W2_t, b2_t)  # f_t is Python float\n",
    "\n",
    "# Print comparison\n",
    "print(\"NumPy output f_np =\", f_np)\n",
    "print(\"Torch output f_t =\", f_t)\n",
    "print(\"Absolute difference =\", abs(f_np - f_t))\n",
    "\n",
    "# Assertion — ensures consistency\n",
    "assert abs(f_np - f_t) < 1e-6, \"NumPy and PyTorch outputs diverge!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b9ceb-cbe0-4732-ba00-d20637185ca5",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "In this notebook we re-implemented the 2-layer neural network forward pass in PyTorch without autograd and aligned it exactly with the NumPy model from Notebook 01. To ensure numerical parity, we matched:\n",
    "\n",
    "- the same dataset (same RNG, same sampling),\n",
    "\n",
    "- the same parameter initialization (small-σ Gaussian weights, zero biases),\n",
    "\n",
    "- the same forward equations (linear → ReLU → linear),\n",
    "\n",
    "- the same loss definition (0.5 * (f - y)**2),\n",
    "\n",
    "- the same dtype (float32 end-to-end).\n",
    "\n",
    "After synchronization, the PyTorch and NumPy outputs matched to 1e-10 precision, confirming full consistency.\n",
    "\n",
    "This establishes a deterministic baseline.\n",
    "Next, in Notebook 03, we introduce PyTorch autograd and compare auto-computed gradients to the manual derivatives from Notebook 01."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
