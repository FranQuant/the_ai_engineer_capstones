# Mini GPT â€” Week 03 Capstone (The AI Engineer)

Diagnostics & implementation of a tiny decoder-only Transformer LM trained in `train_mini_gpt.py`.

---

## ğŸ“˜ Project Overview

This folder contains a complete, from-scratch implementation of a small GPT-style language model, including:

- Scaled Dot-Product Attention  
- Multi-Head Self-Attention  
- Transformer Blocks  
- A compact decoder-only LM (`MiniTransformerLM`)  
- Full training loop with LR warmup + cosine schedule  
- Saved checkpoint (`mini_gpt.pt`)  
- A Jupyter diagnostics suite for interpretability & visualization  

This capstone is fully runnable in Jupyter Lab or Google Colab.

---

## ğŸ“ Repository Structure

The Week-03 capstone is structured intentionally to reflect the build sequence of a tiny decoder-only Transformer:
```text
week03_transformers/
â”‚
â”œâ”€â”€ scaled_dot_product_attention.py      # Step 1: Scaled Dot-Product Attention
â”œâ”€â”€ multihead_attention.py               # Step 2: Multi-Head Attention (MHA)
â”œâ”€â”€ transformer_block.py                 # Step 3: Transformer Block (pre-LN)
â”œâ”€â”€ mini_transformer.py                  # Step 4: Mini Transformer LM
â”‚
â”œâ”€â”€ train_mini_gpt.py                    # Step 5: Training script (from scratch)
â”œâ”€â”€ mini_gpt.pt                          # Saved checkpoint from training
â”‚
â”œâ”€â”€ mini_gpt_diagnostics.ipynb           # Step 6: Diagnostics & visualization suite
â”‚
â””â”€â”€ README_week03_capstone.md            # â† This file

```
---

## ğŸ§  Model Architecture (Minimal Diagram)

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚     Input Token IDs      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Token Embedding + PosEnc â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Transformer Block     â”‚
               â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
               â”‚  â”‚  LayerNorm (ln1)   â”‚  â”‚
               â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
               â”‚  â”‚ Multi-Head Attn    â”‚  â”‚
               â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
               â”‚  â”‚ Residual Add       â”‚  â”‚
               â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
               â”‚  â”‚  LayerNorm (ln2)   â”‚  â”‚
               â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
               â”‚  â”‚ Position FFN       â”‚  â”‚
               â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Final LayerNorm       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚     LM Head (Linear)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                     Next-token logits
```

---

## â–¶ï¸ Training the Mini GPT

Run:

```bash
python train_mini_gpt.py
```

You will see periodic training/validation loss and LR schedule:

```
step   50 | train 30.31 | val 39.99 | lr 3e-4
...
Saved checkpoint to mini_gpt.pt
```

---

## ğŸ§ª Diagnostics Notebook

The notebook `mini_gpt_diagnostics.ipynb` includes:

- Attention heatmaps (per-head + averaged)
- Residual stream norms
- Embedding PCA/TSNE visualization
- Logits histogram + entropy
- Temperature, greedy, and top-k sampling

---

## ğŸ”— Google Colab Link

Open the diagnostics notebook in Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](
https://colab.research.google.com/github/FranQuant/the_ai_engineer/blob/main/capstones/week03_transformers/mini_gpt_diagnostics.ipynb
)


---

## âœ”ï¸ Requirements

Install dependencies:

```bash
pip install matplotlib seaborn scikit-learn torch
```

---

## ğŸ“Œ Notes

This capstone is intentionally small, transparent, and designed for learning:

- No external libraries for the Transformer blocks  
- No hidden helper utilities  
- Fully inspectable attention & residual streams  

The notebook provides interpretability tools similar to those used in real model-debugging workflows.

---