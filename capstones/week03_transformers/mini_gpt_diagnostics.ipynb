{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e906fe1-6535-4bbc-8902-986788ea26bd",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/FranQuant/the_ai_engineer/blob/main/capstones/week03_transformers/mini_gpt_diagnostics.ipynb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cf18b",
   "metadata": {},
   "source": [
    "# Mini GPT — Diagnostic & Visualization Suite\n",
    "\n",
    "Diagnostics for the tiny decoder-only transformer trained in `train_mini_gpt.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1021b",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup\n",
    "- Load the trained checkpoint `mini_gpt.pt`.\n",
    "- Instantiate `MiniTransformerLM` with the same config as the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72df1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from mini_transformer import MiniTransformerLM\n",
    "from transformer_block import TransformerBlock\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from scaled_dot_product_attention import scaled_dot_product_attention\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# -------------------------------------------------------------------------\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Model config (must match train_mini_gpt.py EXACTLY)\n",
    "# -------------------------------------------------------------------------\n",
    "cfg = dict(\n",
    "    d_model=64,         # Match training script\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=4,\n",
    "    max_seq_len=4,\n",
    ")\n",
    "\n",
    "checkpoint_path = Path(\"mini_gpt.pt\")\n",
    "checkpoint_exists = checkpoint_path.exists()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Checkpoint exists: {checkpoint_exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a277d0b",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Utilities\n",
    "Rebuild the character tokenizer and corpus used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Tokenizer Utilities (LOCKED VERSION)\n",
    "# This must match EXACTLY the tiny_text used inside train_mini_gpt.py\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "tiny_text = \"\"\"\n",
    "hello tiny transformer\n",
    "hi tiny transformer\n",
    "hello week three\n",
    "hi transformer\n",
    "\"\"\"\n",
    "\n",
    "# Character vocabulary (sorted, deterministic)\n",
    "chars = sorted(list(set(tiny_text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s: str):\n",
    "    \"\"\"Convert string to list[int].\"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"Convert list[int] or tensor[int] back to string.\"\"\"\n",
    "    return \"\".join(itos[int(i)] for i in ids)\n",
    "\n",
    "# Entire corpus as a flat tensor\n",
    "ids = torch.tensor(encode(tiny_text), dtype=torch.long)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Sample tokens: {chars[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678a932",
   "metadata": {},
   "source": [
    "## 3. Load Model & Causal Mask Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1074b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Load Model & Causal Mask Helper\n",
    "\n",
    "def build_causal_mask(T: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build a lower-triangular causal mask of shape [T, T] with 1s allowed and 0s masked.\n",
    "    The scaled_dot_product_attention() function will convert zeros into -inf internally.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(T, T, device=device))\n",
    "\n",
    "\n",
    "# Instantiate model (must match training config)\n",
    "model = MiniTransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    num_heads=cfg[\"num_heads\"],\n",
    "    d_ff=cfg[\"d_ff\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    max_seq_len=cfg[\"max_seq_len\"],\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint if available\n",
    "if checkpoint_exists:\n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    print(\"Loaded checkpoint mini_gpt.pt\")\n",
    "else:\n",
    "    print(\"Checkpoint not found; using randomly initialized weights (diagnostics only).\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c2fa6",
   "metadata": {},
   "source": [
    "## 4. Attention Heatmaps\n",
    "- Extract per-layer, per-head attention from the model.\n",
    "- Visualize head-wise and averaged attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1463a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Attention Heatmaps\n",
    "# - Extract per-layer, per-head attention from the model.\n",
    "# - Visualize head-wise and averaged attention.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def run_with_attn(model: MiniTransformerLM, input_ids: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
    "    \"\"\"\n",
    "    Manual forward pass to capture attention weights (per layer, per head).\n",
    "\n",
    "    Returns:\n",
    "      logits: [B, T, vocab_size]\n",
    "      attn_weights: list of length num_layers, each [B, H, T, T]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    B, T = input_ids.shape\n",
    "\n",
    "    # Embedding + positional encoding + (optional) embed dropout\n",
    "    x = model.token_embed(input_ids)          # [B,T,D]\n",
    "    x = model.pos_encoding(x)                # sinusoidal PE\n",
    "    x = model.embed_dropout(x)               # same as in MiniTransformerLM.forward\n",
    "\n",
    "    attn_weights = []\n",
    "\n",
    "    for blk in model.blocks:\n",
    "        # Pre-LN\n",
    "        x_norm = blk.ln1(x)                  # [B,T,D]\n",
    "\n",
    "        # Project to Q,K,V and reshape\n",
    "        B_, T_, D_ = x_norm.shape\n",
    "        h = blk.mha.h\n",
    "        dh = blk.mha.dh\n",
    "\n",
    "        q = blk.mha.q_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)  # [B,H,T,Dh]\n",
    "        k = blk.mha.k_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)  # [B,H,T,Dh]\n",
    "        v = blk.mha.v_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)  # [B,H,T,Dh]\n",
    "\n",
    "        # Raw attention scores [B,H,T,T]\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(dh)\n",
    "\n",
    "        # Apply causal / provided mask exactly as in the main path\n",
    "        if attn_mask is not None:\n",
    "            mask = attn_mask\n",
    "            if mask.ndim == 2:                # [T,T]\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)   # [1,1,T,T]\n",
    "            elif mask.ndim == 3:              # [1,T,T] or [B,T,T]\n",
    "                if mask.shape[0] in (1, B_):\n",
    "                    mask = mask.unsqueeze(1)  # [1,1,T,T] or [B,1,T,T]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported 3D mask shape {mask.shape}\")\n",
    "            elif mask.ndim == 4:\n",
    "                # assume [1,1,T,T] or [B,1,T,T]\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mask shape {mask.shape}\")\n",
    "\n",
    "            mask_bool = mask.to(dtype=torch.bool, device=scores.device)\n",
    "            scores = scores.masked_fill(~mask_bool, float(\"-inf\"))\n",
    "\n",
    "        # Stable softmax\n",
    "        scores = scores - scores.max(dim=-1, keepdim=True).values\n",
    "        w = torch.softmax(scores, dim=-1)     # [B,H,T,T]\n",
    "        attn_weights.append(w.detach().cpu())\n",
    "\n",
    "        # Continue forward with the block, re-using the regular MHA\n",
    "        attn_out = blk.mha(x_norm, attn_mask)             # [B,T,D]\n",
    "        attn_out = blk.attn_dropout(attn_out)             # match block forward\n",
    "        x = x + attn_out                                  # residual\n",
    "\n",
    "        x_norm2 = blk.ln2(x)\n",
    "        ffn_out = blk.ffn(x_norm2)\n",
    "        ffn_out = blk.ffn_dropout(ffn_out)\n",
    "        x = x + ffn_out\n",
    "\n",
    "    x = model.final_ln(x)\n",
    "    logits = model.lm_head(x)\n",
    "    return logits, attn_weights\n",
    "\n",
    "\n",
    "# Run a sample through and visualize\n",
    "sample_prompt = \"tiny\"  # length 4, <= cfg[\"max_seq_len\"]\n",
    "assert len(sample_prompt) <= cfg[\"max_seq_len\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ids_prompt = torch.tensor([encode(sample_prompt)], device=device)  # [1,T]\n",
    "    mask = build_causal_mask(ids_prompt.size(1), device)              # [T,T]\n",
    "    logits, attn_maps = run_with_attn(model, ids_prompt, attn_mask=mask)\n",
    "\n",
    "print(f\"Captured {len(attn_maps)} layers of attention; shape layer0: {attn_maps[0].shape}\")\n",
    "\n",
    "layer_idx = 0\n",
    "head_idx = 0\n",
    "attn = attn_maps[layer_idx][0, head_idx].numpy()  # [T,T]\n",
    "tokens = list(sample_prompt)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap=\"magma\", annot=False)\n",
    "plt.title(f\"Layer {layer_idx} Head {head_idx} Attention\")\n",
    "plt.xlabel(\"Key / Value positions\")\n",
    "plt.ylabel(\"Query positions\")\n",
    "plt.show()\n",
    "\n",
    "# Averaged attention over heads\n",
    "plt.figure(figsize=(6, 5))\n",
    "avg_attn = attn_maps[layer_idx].mean(1)[0]        # [T,T]\n",
    "sns.heatmap(avg_attn.numpy(), xticklabels=tokens, yticklabels=tokens, cmap=\"magma\")\n",
    "plt.title(f\"Layer {layer_idx} Averaged Attention\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949d63f",
   "metadata": {},
   "source": [
    "## 5. Residual Stream Diagnostics\n",
    "- Capture norms before/after MHA and FFN per layer.\n",
    "- Plot L2 norms and activation histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc92ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Residual Stream Diagnostics\n",
    "# - Capture norms before/after MHA and FFN per layer.\n",
    "# - Plot L2 norms and activation histograms.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def residual_diagnostics(model: MiniTransformerLM, input_ids: torch.Tensor, attn_mask=None):\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    B, T = input_ids.shape\n",
    "\n",
    "    # Same embedding pipeline as MiniTransformerLM.forward\n",
    "    x = model.token_embed(input_ids)      # [B,T,D]\n",
    "    x = model.pos_encoding(x)            # add sinusoidal PE\n",
    "    x = model.embed_dropout(x)           # (p=0.0 now but keeps API consistent)\n",
    "\n",
    "    norms = []\n",
    "    hists = []\n",
    "\n",
    "    for blk in model.blocks:\n",
    "        # Pre-MHA\n",
    "        x_norm1 = blk.ln1(x)\n",
    "        norms.append({\"pre_mha\": x_norm1.norm(dim=-1).mean().item()})\n",
    "\n",
    "        attn_out = blk.mha(x_norm1, attn_mask)\n",
    "        attn_out = blk.attn_dropout(attn_out)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Pre-FFN\n",
    "        x_norm2 = blk.ln2(x)\n",
    "        norms[-1][\"pre_ffn\"] = x_norm2.norm(dim=-1).mean().item()\n",
    "\n",
    "        ffn_out = blk.ffn(x_norm2)\n",
    "        ffn_out = blk.ffn_dropout(ffn_out)\n",
    "        x = x + ffn_out\n",
    "\n",
    "        # Post-FFN\n",
    "        norms[-1][\"post_ffn\"] = x.norm(dim=-1).mean().item()\n",
    "        hists.append(x.detach().cpu().flatten())\n",
    "\n",
    "    return norms, hists\n",
    "\n",
    "\n",
    "# Use a context whose length <= max_seq_len\n",
    "ids_prompt = torch.tensor([encode(\"tiny\")], device=device)\n",
    "assert ids_prompt.size(1) <= cfg[\"max_seq_len\"]\n",
    "\n",
    "mask = build_causal_mask(ids_prompt.size(1), device)\n",
    "norms, hists = residual_diagnostics(model, ids_prompt, attn_mask=mask)\n",
    "\n",
    "# Plot L2 norms\n",
    "plt.figure(figsize=(6, 4))\n",
    "for key in [\"pre_mha\", \"pre_ffn\", \"post_ffn\"]:\n",
    "    plt.plot([n[key] for n in norms], label=key)\n",
    "plt.title(\"Residual Stream L2 Norms per Layer\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean L2 norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Activation histogram (last layer)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(hists[-1].numpy(), bins=50, alpha=0.8)\n",
    "plt.title(\"Activation Histogram (last layer)\")\n",
    "plt.xlabel(\"Activation value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26a7ca",
   "metadata": {},
   "source": [
    "## 6. Embedding Space Visualization\n",
    "- Project token embeddings to 2D via PCA (fallback to t-SNE if desired).\n",
    "- Annotate characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6. Embedding Space Visualization (LOCKED FINAL VERSION)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Extract token embeddings as numpy array\n",
    "emb = model.token_embed.weight.detach().cpu().numpy()\n",
    "\n",
    "# First try PCA → deterministic, stable\n",
    "try:\n",
    "    proj = PCA(n_components=2, random_state=0).fit_transform(emb)\n",
    "except Exception as e:\n",
    "    print(\"PCA failed, falling back to t-SNE:\", e)\n",
    "    proj = TSNE(\n",
    "        n_components=2,\n",
    "        init=\"random\",\n",
    "        learning_rate=\"auto\",\n",
    "        perplexity=5,\n",
    "        random_state=0\n",
    "    ).fit_transform(emb)\n",
    "\n",
    "# Plot embedding projections\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(proj[:, 0], proj[:, 1], alpha=0.75, s=80)\n",
    "\n",
    "# Annotate each point with its character\n",
    "for i, ch in itos.items():\n",
    "    label = repr(ch).strip(\"'\")          # clean printable character\n",
    "    plt.annotate(label, (proj[i, 0], proj[i, 1]), fontsize=10)\n",
    "\n",
    "plt.title(\"Token Embeddings (2D Projection)\", fontsize=14)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6a083",
   "metadata": {},
   "source": [
    "## 7. Logit / Sampling Diagnostics\n",
    "- Plot logits histogram and entropy for a context.\n",
    "- Show top-k probabilities and temperature effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Logit / Sampling Diagnostics (LOCKED FINAL VERSION)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# A short context (must not exceed max_seq_len=4)\n",
    "context = \"tin\"       # length 3, safe\n",
    "ctx_ids = torch.tensor([encode(context)], device=device)   # [1,T]\n",
    "T = ctx_ids.size(1)\n",
    "\n",
    "# Build causal mask\n",
    "mask = build_causal_mask(T, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(ctx_ids, attn_mask=mask)     # [1,T,V]\n",
    "    last_logits = logits[0, -1]                 # [V]\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    entropy = -(probs * probs.log()).sum().item()\n",
    "\n",
    "print(f\"Entropy (nats): {entropy:.4f}\")\n",
    "print(\"\\nTop-k tokens:\")\n",
    "\n",
    "top_k = min(10, vocab_size)\n",
    "top_probs, top_idx = torch.topk(probs, top_k)\n",
    "\n",
    "for p, idx in zip(top_probs, top_idx):\n",
    "    print(f\"{itos[int(idx)]!r}: {p.item():.3f}\")\n",
    "\n",
    "# Logits histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(last_logits.cpu().numpy(), bins=vocab_size, color=\"salmon\", alpha=0.85)\n",
    "plt.title(\"Last-token logits histogram\")\n",
    "plt.xlabel(\"Logit\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Generation function compatible with max_seq_len=4\n",
    "# -------------------------------------------------------------------------\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    idx: [1, T] initial sequence\n",
    "    Returns: extended sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Always truncate to last max_seq_len tokens\n",
    "        idx_cond = idx[:, -cfg[\"max_seq_len\"]:]        # [1, max_seq_len]\n",
    "\n",
    "        T = idx_cond.size(1)\n",
    "        mask = build_causal_mask(T, idx.device)\n",
    "\n",
    "        logits = model(idx_cond, attn_mask=mask)       # [1,T,V]\n",
    "        logits = logits[:, -1, :] / temperature        # [1,V]\n",
    "\n",
    "        if top_k is not None:\n",
    "            k = min(top_k, logits.size(-1))\n",
    "            v, ix = torch.topk(logits, k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)          # [1,V]\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "\n",
    "        idx = torch.cat([idx, next_id], dim=1)         # extend sequence\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Generate samples\n",
    "# -------------------------------------------------------------------------\n",
    "start = torch.tensor([[random.randint(0, vocab_size - 1)]], device=device)\n",
    "\n",
    "samples = {\n",
    "    \"greedy\":   generate(model, start.clone(), 40, temperature=1.0, top_k=1),\n",
    "    \"temp_0.8\": generate(model, start.clone(), 40, temperature=0.8),\n",
    "    \"temp_1.2\": generate(model, start.clone(), 40, temperature=1.2),\n",
    "    \"topk8\":    generate(model, start.clone(), 40, temperature=0.9, top_k=8),\n",
    "}\n",
    "\n",
    "for name, seq in samples.items():\n",
    "    print(f\"\\n=== Sample ({name}) ===\")\n",
    "    print(decode(seq[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11693fd",
   "metadata": {},
   "source": [
    "## 8. Conclusions / Notes\n",
    "- Use attention heatmaps to verify causal masking and focus patterns.\n",
    "- Monitor residual norms for stability; large growth may indicate learning-rate issues.\n",
    "- Embedding projections reveal clustering by character class.\n",
    "- Logit entropy and top-k give a quick sense of calibration; sampling comparisons show temperature/top-k effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
