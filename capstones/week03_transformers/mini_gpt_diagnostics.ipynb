{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e906fe1-6535-4bbc-8902-986788ea26bd",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/FranQuant/the_ai_engineer/blob/main/capstones/week03_transformers/mini_gpt_diagnostics.ipynb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cf18b",
   "metadata": {},
   "source": [
    "# Mini GPT â€” Diagnostic & Visualization Suite\n",
    "\n",
    "Diagnostics for the tiny decoder-only transformer trained in `train_mini_gpt.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1021b",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup\n",
    "- Load the trained checkpoint `mini_gpt.pt`.\n",
    "- Instantiate `MiniTransformerLM` with the same config as the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72df1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from mini_transformer import MiniTransformerLM\n",
    "from transformer_block import TransformerBlock\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from scaled_dot_product_attention import scaled_dot_product_attention\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Training config (must match train_mini_gpt.py)\n",
    "cfg = dict(\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=2,\n",
    "    max_seq_len=128,\n",
    ")\n",
    "\n",
    "checkpoint_path = Path(\"mini_gpt.pt\")\n",
    "checkpoint_exists = checkpoint_path.exists()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Checkpoint exists: {checkpoint_exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a277d0b",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Utilities\n",
    "Rebuild the character tokenizer and corpus used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Attention is all you need.\n",
    "Transformers use self-attention to model dependencies.\n",
    "This is a tiny training corpus for the Week 03 capstone.\n",
    "\"\"\"\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join([itos[int(i)] for i in ids])\n",
    "\n",
    "ids = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Sample tokens: {chars[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678a932",
   "metadata": {},
   "source": [
    "## 3. Load Model & Causal Mask Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1074b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask(T: int, device: torch.device):\n",
    "    return torch.tril(torch.ones(T, T, device=device))\n",
    "\n",
    "# Instantiate model\n",
    "model = MiniTransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    num_heads=cfg[\"num_heads\"],\n",
    "    d_ff=cfg[\"d_ff\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    max_seq_len=cfg[\"max_seq_len\"],\n",
    ").to(device)\n",
    "\n",
    "if checkpoint_exists:\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(\"Loaded checkpoint mini_gpt.pt\")\n",
    "else:\n",
    "    print(\"Checkpoint not found; using randomly initialized weights (diagnostics only).\")\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c2fa6",
   "metadata": {},
   "source": [
    "## 4. Attention Heatmaps\n",
    "- Extract per-layer, per-head attention from the model.\n",
    "- Visualize head-wise and averaged attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1463a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_attn(model: MiniTransformerLM, input_ids: torch.Tensor, attn_mask: torch.Tensor | None = None):\n",
    "    \"\"\"Manual forward to capture attention weights (per layer, per head).\"\"\"\n",
    "    model.eval()\n",
    "    B, T = input_ids.shape\n",
    "    device = input_ids.device\n",
    "    pos = torch.arange(T, device=device).unsqueeze(0)\n",
    "    x = model.token_embed(input_ids) + model.pos_embed(pos)\n",
    "\n",
    "    attn_weights = []  # list of [B,H,T,T]\n",
    "    tokens = input_ids\n",
    "\n",
    "    for blk in model.blocks:\n",
    "        x_norm = blk.ln1(x)\n",
    "        B_, T_, D_ = x_norm.shape\n",
    "        h = blk.mha.h\n",
    "        dh = blk.mha.dh\n",
    "\n",
    "        q = blk.mha.q_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)  # [B,H,T,Dh]\n",
    "        k = blk.mha.k_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)\n",
    "        v = blk.mha.v_proj(x_norm).view(B_, T_, h, dh).transpose(1, 2)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(dh)  # [B,H,T,T]\n",
    "        if attn_mask is not None:\n",
    "            mask = attn_mask\n",
    "            if mask.ndim == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            if mask.ndim == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores + (mask == 0) * (-1e9)\n",
    "        scores = scores - scores.max(dim=-1, keepdim=True).values\n",
    "        w = torch.softmax(scores, dim=-1)  # [B,H,T,T]\n",
    "        attn_weights.append(w.detach().cpu())\n",
    "\n",
    "        # Continue forward\n",
    "        attn_out = blk.mha(x_norm, attn_mask)\n",
    "        x = x + attn_out\n",
    "        x_norm2 = blk.ln2(x)\n",
    "        ffn_out = blk.ffn(x_norm2)\n",
    "        x = x + ffn_out\n",
    "\n",
    "    x = model.final_ln(x)\n",
    "    logits = model.lm_head(x)\n",
    "    return logits, attn_weights\n",
    "\n",
    "\n",
    "sample_prompt = \"Attention is\"\n",
    "with torch.no_grad():\n",
    "    ids_prompt = torch.tensor([encode(sample_prompt)], device=device)\n",
    "    mask = build_causal_mask(ids_prompt.size(1), device)\n",
    "    logits, attn_maps = run_with_attn(model, ids_prompt, attn_mask=mask)\n",
    "\n",
    "print(f\"Captured {len(attn_maps)} layers of attention; shape layer0: {attn_maps[0].shape}\")\n",
    "\n",
    "# Visualize first layer, head 0\n",
    "layer_idx = 0\n",
    "head_idx = 0\n",
    "attn = attn_maps[layer_idx][0, head_idx].numpy()\n",
    "tokens = list(sample_prompt)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap=\"magma\", annot=False)\n",
    "plt.title(f\"Layer {layer_idx} Head {head_idx} Attention\")\n",
    "plt.xlabel(\"Key / Value positions\")\n",
    "plt.ylabel(\"Query positions\")\n",
    "plt.show()\n",
    "\n",
    "# Averaged attention per layer\n",
    "plt.figure(figsize=(6, 5))\n",
    "avg_attn = attn_maps[layer_idx].mean(1)[0]\n",
    "sns.heatmap(avg_attn.numpy(), xticklabels=tokens, yticklabels=tokens, cmap=\"magma\")\n",
    "plt.title(f\"Layer {layer_idx} Averaged Attention\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949d63f",
   "metadata": {},
   "source": [
    "## 5. Residual Stream Diagnostics\n",
    "- Capture norms before/after MHA and FFN per layer.\n",
    "- Plot L2 norms and activation histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc92ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_diagnostics(model: MiniTransformerLM, input_ids: torch.Tensor, attn_mask=None):\n",
    "    model.eval()\n",
    "    B, T = input_ids.shape\n",
    "    pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
    "    x = model.token_embed(input_ids) + model.pos_embed(pos)\n",
    "    norms = []\n",
    "    hists = []\n",
    "    for blk in model.blocks:\n",
    "        x_norm1 = blk.ln1(x)\n",
    "        norms.append({\"pre_mha\": x_norm1.norm(dim=-1).mean().item()})\n",
    "        attn_out = blk.mha(x_norm1, attn_mask)\n",
    "        x = x + attn_out\n",
    "        x_norm2 = blk.ln2(x)\n",
    "        norms[-1][\"pre_ffn\"] = x_norm2.norm(dim=-1).mean().item()\n",
    "        ffn_out = blk.ffn(x_norm2)\n",
    "        x = x + ffn_out\n",
    "        norms[-1][\"post_ffn\"] = x.norm(dim=-1).mean().item()\n",
    "        hists.append(x.detach().cpu().flatten())\n",
    "    return norms, hists\n",
    "\n",
    "\n",
    "ids_prompt = torch.tensor([encode(\"Transformers\")], device=device)\n",
    "mask = build_causal_mask(ids_prompt.size(1), device)\n",
    "norms, hists = residual_diagnostics(model, ids_prompt, attn_mask=mask)\n",
    "\n",
    "# Plot L2 norms\n",
    "plt.figure(figsize=(6,4))\n",
    "for key in [\"pre_mha\", \"pre_ffn\", \"post_ffn\"]:\n",
    "    plt.plot([n[key] for n in norms], label=key)\n",
    "plt.title(\"Residual Stream L2 Norms per Layer\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean L2 norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Activation histogram (last layer)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(hists[-1].numpy(), bins=50, color=\"steelblue\", alpha=0.8)\n",
    "plt.title(\"Activation Histogram (last layer)\")\n",
    "plt.xlabel(\"Activation value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26a7ca",
   "metadata": {},
   "source": [
    "## 6. Embedding Space Visualization\n",
    "- Project token embeddings to 2D via PCA (fallback to t-SNE if desired).\n",
    "- Annotate characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.token_embed.weight.detach().cpu().numpy()\n",
    "try:\n",
    "    proj = PCA(n_components=2).fit_transform(emb)\n",
    "except Exception:\n",
    "    proj = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\").fit_transform(emb)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.7)\n",
    "for i, ch in itos.items():\n",
    "    plt.annotate(repr(ch).strip(\"'\"), (proj[i,0], proj[i,1]), fontsize=9)\n",
    "plt.title(\"Token Embeddings (2D projection)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6a083",
   "metadata": {},
   "source": [
    "## 7. Logit / Sampling Diagnostics\n",
    "- Plot logits histogram and entropy for a context.\n",
    "- Show top-k probabilities and temperature effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Attention is\"\n",
    "ctx_ids = torch.tensor([encode(context)], device=device)\n",
    "mask = build_causal_mask(ctx_ids.size(1), device)\n",
    "with torch.no_grad():\n",
    "    logits = model(ctx_ids, attn_mask=mask)\n",
    "    last_logits = logits[0, -1]\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    entropy = -(probs * probs.log()).sum().item()\n",
    "\n",
    "top_k = 10\n",
    "top_probs, top_idx = torch.topk(probs, top_k)\n",
    "print(\"Entropy (nats):\", entropy)\n",
    "print(\"Top-k tokens:\")\n",
    "for p, idx in zip(top_probs, top_idx):\n",
    "    print(f\"{itos[int(idx)]!r}: {p.item():.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(last_logits.cpu().numpy(), bins=50, color=\"salmon\", alpha=0.8)\n",
    "plt.title(\"Last-token logits histogram\")\n",
    "plt.xlabel(\"Logit\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -cfg[\"max_seq_len\"] :]\n",
    "        T = idx_cond.size(1)\n",
    "        mask = build_causal_mask(T, idx.device)\n",
    "        logits = model(idx_cond, attn_mask=mask)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "start = torch.tensor([[random.randint(0, vocab_size - 1)]], device=device)\n",
    "samples = {\n",
    "    \"greedy\": generate(model, start.clone(), 80, temperature=1.0, top_k=1),\n",
    "    \"temp_0.8\": generate(model, start.clone(), 80, temperature=0.8),\n",
    "    \"temp_1.2\": generate(model, start.clone(), 80, temperature=1.2),\n",
    "    \"topk8\": generate(model, start.clone(), 80, temperature=0.9, top_k=8),\n",
    "}\n",
    "\n",
    "for name, seq in samples.items():\n",
    "    print(f\"\\nSample ({name}):\\n{decode(seq[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11693fd",
   "metadata": {},
   "source": [
    "## 8. Conclusions / Notes\n",
    "- Use attention heatmaps to verify causal masking and focus patterns.\n",
    "- Monitor residual norms for stability; large growth may indicate learning-rate issues.\n",
    "- Embedding projections reveal clustering by character class.\n",
    "- Logit entropy and top-k give a quick sense of calibration; sampling comparisons show temperature/top-k effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
